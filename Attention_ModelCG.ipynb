{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":156584,"status":"ok","timestamp":1702335611119,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"HN70G7tImwrz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc0c8369-08cd-43dc-e08d-da75139529e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m744.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q\n","\n","!pip install wandb --quiet\n","!pip install python-Levenshtein -q\n","\n","!pip install torchsummaryX -q\n","!pip install mne -q"],"id":"HN70G7tImwrz"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4936,"status":"ok","timestamp":1702335616033,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"AfMLy3CQBrKA","outputId":"eb0d76f3-44a9-4845-e92d-f2492349de0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device:  cuda\n"]}],"source":["import torch\n","import torchaudio\n","from torch import nn, Tensor\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","# import torchsummary\n","\n","import numpy as np\n","import os\n","\n","import gc\n","import time\n","\n","import pandas as pd\n","from tqdm.notebook import tqdm as blue_tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json\n","\n","import math\n","from typing import Optional, List\n","\n","\n","#imports for decoding and distance calculation\n","try:\n","    import wandb\n","    import torchsummaryX\n","    import Levenshtein\n","except:\n","    print(\"Didnt install some/all imports\")\n","\n","import sklearn\n","import mne\n","\n","if torch.backends.mps.is_available():\n","    device = \"mps\"\n","elif torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","DEVICE=device\n","print(\"Device: \", device)"],"id":"AfMLy3CQBrKA"},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1702335616033,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"Vvn97zh-ARFV"},"outputs":[],"source":["config = dict (\n","    batch_size          = 36,\n","    epochs              = 100,\n","    learning_rate       = 1e-4, # per write up\n","    weight_decay        = 0,\n",")\n","\n","# Suggested transforms per the write up\n","transforms = torch.nn.Sequential(\n","    torchaudio.transforms.TimeMasking(time_mask_param=225),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=6)\n",")"],"id":"Vvn97zh-ARFV"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1702335616034,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"03522879","outputId":"64b343be-6405-4073-d74c-76b8d42891ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Length of vocab : 31\n","Vocab           : ['<pad>', '<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ']\n","PAD_TOKEN       : 0\n","SOS_TOKEN       : 1\n","EOS_TOKEN       : 2\n"]}],"source":["VOCAB = [\n","    '<pad>', '<sos>', '<eos>',\n","    'A',   'B',    'C',    'D',\n","    'E',   'F',    'G',    'H',\n","    'I',   'J',    'K',    'L',\n","    'M',   'N',    'O',    'P',\n","    'Q',   'R',    'S',    'T',\n","    'U',   'V',    'W',    'X',\n","    'Y',   'Z',    \"'\",    ' ',\n","]\n","\n","VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n","\n","PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n","SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n","EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n","\n","print(f\"Length of vocab : {len(VOCAB)}\")\n","print(f\"Vocab           : {VOCAB}\")\n","print(f\"PAD_TOKEN       : {PAD_TOKEN}\")\n","print(f\"SOS_TOKEN       : {SOS_TOKEN}\")\n","print(f\"EOS_TOKEN       : {EOS_TOKEN}\")"],"id":"03522879"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19426,"status":"ok","timestamp":1702335635435,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"jybQUlpVjCkX","outputId":"4fee101a-22d8-49da-fb92-c87f57452e77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive # Link your drive if you are a colab user\n","drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her"],"id":"jybQUlpVjCkX"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1702335636493,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"pHYLRN_f3pS2","outputId":"acd84ab0-7d1b-4908-bc6c-a12ee23f12db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0       Word  Segment     onset    offset  Order  LogFreq  \\\n","0           0      Alice        1  0.046000  0.608721      1     8.65   \n","1           1        was        1  0.562721  0.830543      2    14.56   \n","2           2  beginning        1  0.784543  1.302929      3    10.69   \n","3           3         to        1  1.256929  1.398925      4    16.35   \n","4           4        get        1  1.352925  1.662327      5    13.79   \n","\n","   LogFreq_Prev  LogFreq_Next      SndPower    Length  Position  Sentence  \\\n","0          0.00         14.56  3.620000e-07  0.562721         1         1   \n","1          8.65         10.69  3.840000e-09  0.267822         2         1   \n","2         14.56         16.35  3.690000e-09  0.518386         3         1   \n","3         10.69         13.79  3.970000e-09  0.141996         4         1   \n","4         16.35         13.28  3.770000e-09  0.309402         5         1   \n","\n","   IsLexical     NGRAM       RNN       CFG  \\\n","0        1.0  3.226499  3.126175  2.312348   \n","1        0.0  0.905229  1.691128  1.357460   \n","2        1.0  4.446766  4.100771  5.626722   \n","3        0.0  2.537495  3.833313  5.939201   \n","4        0.0  1.023137  1.013076  2.697304   \n","\n","                                     phonemes  \n","0                    ['AE1', 'L', 'IH0', 'S']  \n","1                           ['W', 'AH0', 'Z']  \n","2  ['B', 'IH0', 'G', 'IH1', 'N', 'IH0', 'NG']  \n","3                                ['T', 'AH0']  \n","4                           ['G', 'IH1', 'T']  "],"text/html":["\n","  <div id=\"df-fd198a5f-039b-4f80-88b7-ca99019c9085\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Word</th>\n","      <th>Segment</th>\n","      <th>onset</th>\n","      <th>offset</th>\n","      <th>Order</th>\n","      <th>LogFreq</th>\n","      <th>LogFreq_Prev</th>\n","      <th>LogFreq_Next</th>\n","      <th>SndPower</th>\n","      <th>Length</th>\n","      <th>Position</th>\n","      <th>Sentence</th>\n","      <th>IsLexical</th>\n","      <th>NGRAM</th>\n","      <th>RNN</th>\n","      <th>CFG</th>\n","      <th>phonemes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Alice</td>\n","      <td>1</td>\n","      <td>0.046000</td>\n","      <td>0.608721</td>\n","      <td>1</td>\n","      <td>8.65</td>\n","      <td>0.00</td>\n","      <td>14.56</td>\n","      <td>3.620000e-07</td>\n","      <td>0.562721</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>3.226499</td>\n","      <td>3.126175</td>\n","      <td>2.312348</td>\n","      <td>['AE1', 'L', 'IH0', 'S']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>was</td>\n","      <td>1</td>\n","      <td>0.562721</td>\n","      <td>0.830543</td>\n","      <td>2</td>\n","      <td>14.56</td>\n","      <td>8.65</td>\n","      <td>10.69</td>\n","      <td>3.840000e-09</td>\n","      <td>0.267822</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.905229</td>\n","      <td>1.691128</td>\n","      <td>1.357460</td>\n","      <td>['W', 'AH0', 'Z']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>beginning</td>\n","      <td>1</td>\n","      <td>0.784543</td>\n","      <td>1.302929</td>\n","      <td>3</td>\n","      <td>10.69</td>\n","      <td>14.56</td>\n","      <td>16.35</td>\n","      <td>3.690000e-09</td>\n","      <td>0.518386</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>4.446766</td>\n","      <td>4.100771</td>\n","      <td>5.626722</td>\n","      <td>['B', 'IH0', 'G', 'IH1', 'N', 'IH0', 'NG']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>to</td>\n","      <td>1</td>\n","      <td>1.256929</td>\n","      <td>1.398925</td>\n","      <td>4</td>\n","      <td>16.35</td>\n","      <td>10.69</td>\n","      <td>13.79</td>\n","      <td>3.970000e-09</td>\n","      <td>0.141996</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>2.537495</td>\n","      <td>3.833313</td>\n","      <td>5.939201</td>\n","      <td>['T', 'AH0']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>get</td>\n","      <td>1</td>\n","      <td>1.352925</td>\n","      <td>1.662327</td>\n","      <td>5</td>\n","      <td>13.79</td>\n","      <td>16.35</td>\n","      <td>13.28</td>\n","      <td>3.770000e-09</td>\n","      <td>0.309402</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.023137</td>\n","      <td>1.013076</td>\n","      <td>2.697304</td>\n","      <td>['G', 'IH1', 'T']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd198a5f-039b-4f80-88b7-ca99019c9085')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fd198a5f-039b-4f80-88b7-ca99019c9085 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fd198a5f-039b-4f80-88b7-ca99019c9085');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e9b19c14-85f8-46a8-a661-01b61ccec697\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9b19c14-85f8-46a8-a661-01b61ccec697')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e9b19c14-85f8-46a8-a661-01b61ccec697 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}],"source":["# Load in the new and glorious Phoneme-added dataframe\n","import pandas as pd\n","data_csv =pd.read_csv(\"drive/MyDrive/brennan data/AliceChapterOne-EEG-Phonemes.csv\")\n","data_csv.head()"],"id":"pHYLRN_f3pS2"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1702335636493,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"_67FUEEpDRtH","outputId":"e05b703a-a257-4633-db29-f8679af09a79"},"outputs":[{"output_type":"stream","name":"stdout","text":["['ALICE WAS BEGINNING TO GET VERY TIRED OF SITTING BY HER SISTER ON THE BANK AND OF HAVING NOTHING TO DO ONCE OR TWICE SHE PEEPED INTO THE BOOK HER SISTER WAS READING BUT IT HAD NO PICTURES OR CONVERSATIONS IN IT AND WHAT THE USE OF A BOOK THOUGHT ALICE WITHOUT PICTURES OR CONVERSATION SO SHE WAS CONSIDERING IN HER OWN MIND AS WELL AS SHE COULD FOR THE HOT DAY MADE HER FEEL VERY SLEEPY AND STUPID WHETHER THE PLEASURE OF MAKING A DAISY CHAIN WOULD BE WORTH THE TROUBLE OF GETTING UP AND PICKING THE DAISIES WHEN SUDDENLY A WHITE RABBIT WITH PINK EYES RAN CLOSE BY HER THERE WAS NOTHING SO VERY REMARKABLE IN THAT NOR DID ALICE THINK IT SO VERY MUCH OUT OF THE WAY TO HEAR THE RABBIT SAY TO ITSELF OH DEAR OH DEAR I SHALL BE LATE WHEN SHE THOUGHT IT OVER AFTERWARDS IT OCCURRED TO HER THAT SHE OUGHT TO HAVE WONDERED AT THIS BUT AT THE TIME IT ALL SEEMED QUITE NATURAL', 'BUT WHEN THE RABBIT ACTUALLY TOOK A WATCH OUT OF ITS POCKET AND LOOKED AT IT AND THEN HURRIED ON ALICE STARTED TO HER FEET FOR IT FLASHED ACROSS HER MIND THAT SHE NEVER BEFORE SEEN A RABBIT WITH EITHER A POCKET OR A WATCH TO TAKE OUT OF IT AND BURNING WITH CURIOSITY SHE RAN ACROSS THE FIELD AFTER IT AND FORTUNATELY WAS JUST IN TIME TO SEE IT POP DOWN A LARGE RABBIT HOLE UNDER THE HEDGE IN ANOTHER MOMENT DOWN WENT ALICE AFTER IT NEVER ONCE CONSIDERING HOW IN THE WORLD SHE WAS TO GET OUT AGAIN THE RABBIT HOLE WENT STRAIGHT ON LIKE A TUNNEL FOR SOME WAY AND THEN DIPPED SUDDENLY DOWN SO SUDDENLY THAT ALICE HAD NOT A MOMENT TO THINK ABOUT STOPPING HERSELF BEFORE SHE FOUND HERSELF FALLING DOWN A VERY DEEP WELL EITHER THE WELL WAS VERY DEEP OR SHE FELL VERY SLOWLY FOR SHE HAD PLENTY OF TIME AS SHE WENT DOWN TO LOOK ABOUT HER AND TO WONDER WHAT WAS GOING TO HAPPEN NEXT', 'FIRST SHE TRIED TO LOOK DOWN AND MAKE OUT WHAT SHE WAS COMING TO BUT IT WAS TOO DARK TO SEE ANYTHING THEN SHE LOOKED AT THE SIDES OF THE WELL AND NOTICED THAT THEY WERE FILLED WITH CUPBOARDS AND BOOKSHELVES HERE AND THERE SHE SAW MAPS AND PICTURES HUNG UP ON PEGS SHE TOOK DOWN A JAR FROM ONE OF THE SHELVES AS SHE PASSED IT WAS LABELLED ORANGE MARMALADE BUT TO HER GREAT DISAPPOINTMENT IT WAS EMPTY SHE DID NOT LIKE TO DROP THE JAR FOR FEAR OF KILLING SOMEBODY SO SHE MANAGED TO PUT IT INTO ONE OF THE CUPBOARDS AS SHE FELL PAST IT WELL THOUGHT ALICE TO HERSELF AFTER SUCH A FALL AS THIS I SHALL THINK NOTHING OF TUMBLING DOWN STAIRS HOW BRAVE THEY ALL THINK ME AT HOME WHY I WOULD SAY ANYTHING ABOUT IT EVEN IF I FELL OFF THE TOP OF THE HOUSE WHICH WAS VERY LIKELY TRUE DOWN DOWN DOWN WOULD THE FALL NEVER COME TO AN END I WONDER HOW MANY MILES I VE FALLEN BY THIS TIME SHE SAID ALOUD', 'MUST BE GETTING SOMEWHERE NEAR THE CENTER OF THE EARTH LET ME SEE THAT WOULD BE FOUR THOUSAND MILES DOWN I THINK FOR YOU SEE ALICE HAD LEARNT SEVERAL THINGS OF THIS SORT IN HER LESSONS IN THE SCHOOL ROOM AND THOUGH THIS WAS NOT A VERY GOOD OPPORTUNITY FOR SHOWING OFF HER KNOWLEDGE AS THERE WAS NO ONE TO LISTEN TO HER STILL IT WAS GOOD PRACTICE TO SAY IT OVER YES THAT S ABOUT THE RIGHT DISTANCE BUT THEN I WONDER WHAT LATITUDE OR LONGITUDE I VE GOT TO ALICE HAD NO IDEA WHAT LATITUDE WAS OR LONGITUDE EITHER BUT THOUGHT THEY WERE NICE GRAND WORDS TO SAY PRESENTLY SHE BEGAN AGAIN I WONDER IF I SHALL FALL RIGHT THROUGH THE EARTH HOW FUNNY IT SEEM TO COME OUT AMONG THE PEOPLE THAT WALK WITH THEIR HEADS DOWNWARD THE ANTIPATHIES I THINK SHE WAS RATHER GLAD THERE WAS NO ONE LISTENING THIS TIME AS IT DID SOUND AT ALL THE RIGHT WORD BUT I SHALL HAVE TO ASK THEM WHAT THE NAME OF THE COUNTRY IS YOU KNOW PLEASE MAAM IS THIS NEW ZEALAND OR AUSTRALIA AND SHE TRIED TO AS SHE SPOKE FANCY AS YOU RE FALLING THROUGH THE AIR DO YOU THINK YOU COULD MANAGE IT', 'AND WHAT AN IGNORANT LITTLE GIRL SHE THINK ME FOR ASKING NO IT NEVER DO TO ASK PERHAPS I SHALL SEE IT WRITTEN UP SOMEWHERE DOWN DOWN DOWN THERE WAS NOTHING ELSE TO DO SO ALICE SOON BEGAN TALKING AGAIN DINAH MISS ME VERY MUCH TONIGHT I SHOULD THINK DINAH WAS THE CAT I HOPE THEY REMEMBER HER SAUCER OF MILK AT TEA TIME DINAH MY DEAR I WISH YOU WERE DOWN HERE WITH ME THERE ARE NO MICE IN THE AIR I M AFRAID BUT YOU MIGHT CATCH A BAT AND THAT S VERY LIKE A MOUSE YOU KNOW BUT DO CATS EAT BATS I WONDER AND HERE ALICE BEGAN TO GET RATHER SLEEPY AND WENT ON SAYING TO HERSELF IN A DREAMY SORT OF WAY DO CATS EAT BATS DO CATS EAT BATS AND SOMETIMES DO BATS EAT CATS FOR YOU SEE AS SHE COULD ANSWER EITHER QUESTION IT DID MUCH MATTER WHICH WAY SHE PUT IT SHE FELT THAT SHE WAS DOZING OFF AND HAD JUST BEGUN TO DREAM THAT SHE WAS WALKING HAND IN HAND WITH DINAH AND SAYING TO HER VERY EARNESTLY', 'NOW DINAH TELL ME THE TRUTH DID YOU EVER EAT A BAT WHEN SUDDENLY THUMP THUMP DOWN SHE CAME UPON A HEAP OF STICKS AND DRY LEAVES AND THE FALL WAS OVER ALICE WAS NOT A BIT HURT AND SHE JUMPED UP ONTO HER FEET IN A MOMENT SHE LOOKED UP BUT IT WAS ALL DARK OVERHEAD BEFORE HER WAS ANOTHER LONG PASSAGE AND THE WHITE RABBIT WAS STILL IN SIGHT HURRYING DOWN IT THERE WAS NOT A MOMENT TO BE LOST AWAY WENT ALICE LIKE THE WIND AND WAS JUST IN TIME TO HEAR IT SAY AS IT TURNED A CORNER OH MY EARS AND WHISKERS HOW LATE IT S GETTING SHE WAS CLOSE BEHIND IT WHEN SHE TURNED THE CORNER BUT THE RABBIT WAS NO LONGER TO BE SEEN SHE FOUND HERSELF IN A LONG LOW HALL WHICH WAS LIT UP BY A ROW OF LAMPS HANGING FROM THE ROOF THERE WERE DOORS ALL AROUND THE HALL BUT THEY WERE ALL LOCKED AND WHEN ALICE HAD BEEN ALL THE WAY DOWN ONE SIDE AND UP THE OTHER TRYING EVERY DOOR SHE WALKED SADLY DOWN THE MIDDLE WONDERING HOW SHE WAS EVER TO GET OUT AGAIN', \"SUDDENLY SHE CAME UPON A LITTLE THREE LEGGED TABLE ALL MADE OF SOLID GLASS THERE WAS NOTHING ON IT EXCEPT A TINY GOLDEN KEY AND ALICE'S FIRST THOUGHT WAS THAT IT MIGHT BELONG TO ONE OF THE DOORS OF THE HALL BUT ALAS EITHER THE LOCKS WERE TOO LARGE OR THE KEY WAS TOO SMALL BUT AT ANY RATE IT WOULD NOT OPEN ANY OF THEM HOWEVER ON THE SECOND TIME ROUND SHE CAME UPON A LOW CURTAIN SHE HAD NOT NOTICED BEFORE AND BEHIND IT WAS A LITTLE DOOR ABOUT FIFTEEN INCHES HIGH SHE TRIED THE LITTLE GOLDEN KEY IN THE LOCK AND TO HER GREAT DELIGHT IT FITTED ALICE OPENED THE DOOR AND FOUND IT LED INTO A SMALL PASSAGE NOT MUCH LARGER THAN A RAT HOLE SHE KNELT DOWN AND LOOKED ALONG THE PASSAGE INTO THE GARDEN YOU EVER SAW HOW SHE LONGED TO GET OUT OF THAT DARK HALL AND WANDER ABOUT AMONG THOSE BEDS OF BRIGHT FLOWERS AND THOSE COOL FOUNTAINS BUT SHE COULD NOT EVEN GET HER HEAD THOUGH THE DOORWAY\", 'AND EVEN IF MY HEAD WOULD GO THROUGH THOUGHT POOR ALICE IT WOULD BE OF VERY LITTLE USE WITHOUT MY SHOULDERS OH HOW I WISH I COULD SHUT UP LIKE A TELESCOPE I THINK I COULD IF I ONLY KNOW HOW TO BEGIN FOR YOU SEE SO MANY OUT OF THE WAY THINGS HAD HAPPENED LATELY THAT ALICE HAD BEGUN TO THINK THAT VERY FEW THINGS INDEED WERE REALLY IMPOSSIBLE THERE SEEMED TO BE NO USE IN WAITING BY THE LITTLE DOOR SO SHE WENT BACK TO THE TABLE HALF HOPING SHE MIGHT FIND ANOTHER KEY ON IT OR AT ANY RATE A BOOK OF RULES FOR SHUTTING PEOPLE UP LIKE TELESCOPES THIS TIME SHE FOUND A LITTLE BOTTLE ON IT WHICH CERTAINLY WAS NOT HERE BEFORE SAID ALICE AND ROUND THE NECK OF THE BOTTLE WAS A PAPER LABEL WITH THE WORDS DRINK ME BEAUTIFULLY PRINTED ON IT IN LARGE LETTERS IT WAS ALL VERY WELL TO SAY DRINK ME BUT THE WISE LITTLE ALICE WAS NOT GOING TO DO THAT IN A HURRY', 'NO I LOOK FIRST SHE SAID AND SEE WHETHER IT S MARKED POISON OR NOT FOR SHE HAD READ SEVERAL NICE LITTLE HISTORIES ABOUT CHILDREN WHO GOTTEN BURNT AND EATEN UP BY WILD BEASTS AND OTHER UNPLEASANT THINGS ALL BECAUSE THEY WOULD NOT REMEMBER THE SIMPLE RULES THEIR FRIENDS HAD TAUGHT THEM SUCH AS THAT A RED HOT POKER WILL BURN YOU IF YOU HOLD IT TOO LONG AND THAT IF YOU CUT YOUR FINGER VERY DEEPLY WITH A KNIFE IT USUALLY BLEEDS AND SHE HAD NEVER FORGOTTEN THAT IF YOU DRINK MUCH FROM A BOTTLE MARKED POISON IT ALMOST CERTAIN TO DISAGREE WITH YOU SOONER OR LATER HOWEVER THIS BOTTLE WAS NOT MARKED POISON SO ALICE VENTURED TO TASTE IT AND FINDING IT VERY NICE IT HAD IN FACT A SORT OF MIXED FLAVOR OF CHERRY TART CUSTARD PINEAPPLE ROAST TURKEY TOFFEE AND HOT BUTTERED TOAST SHE VERY SOON FINISHED IT OFF', 'WHAT A CURIOUS FEELING SAID ALICE I MUST BE SHUTTING UP LIKE A TELESCOPE AND SO IT WAS INDEED SHE WAS NOW ONLY TEN INCHES HIGH AND HER FACE BRIGHTENED UP AT THE THOUGHT THAT SHE WAS NOW THE RIGHT SIZE FOR GOING THROUGH THE LITTLE DOOR INTO THAT LOVELY GARDEN FIRST HOWEVER SHE WAITED FOR A FEW MINUTES TO SEE IF SHE WAS GOING TO SHRINK ANY FURTHER SHE FELT A LITTLE NERVOUS ABOUT THIS FOR IT MIGHT END YOU KNOW SAID ALICE TO HERSELF IN MY GOING OUT ALTOGETHER LIKE A CANDLE I WONDER WHAT I SHOULD BE LIKE THEN AND SHE TRIED TO FANCY WHAT THE FLAME OF A CANDLE IS LIKE AFTER THE CANDLE IS BLOWN OUT FOR SHE COULD NOT REMEMBER HAVING EVER SEEN SUCH A THING AFTER A WHILE FINDING THAT NOTHING MORE HAPPENED SHE DECIDED ON GOING INTO THE GARDEN AT ONCE BUT ALAS FOR POOR ALICE WHEN SHE GOT TO THE DOOR SHE FOUND SHE D FORGOTTEN THE LITTLE GOLDEN KEY AND WHEN SHE WENT BACK TO THE TABLE FOR IT SHE FOUND SHE COULD NOT POSSIBLY REACH IT', 'SHE COULD SEE IT QUITE PLAINLY THROUGH THE GLASS AND SHE TRIED HER BEST TO CLIMB UP ONE OF THE LEGS OF THE TABLE BUT IT WAS TOO SLIPPERY AND WHEN SHE HAD TIRED HERSELF OUT WITH TRYING THE POOR LITTLE THING SAT DOWN AND CRIED COME THERE S NO USE CRYING LIKE THAT SAID ALICE TO HERSELF RATHER SHARPLY I ADVISE YOU TO LEAVE OFF THIS MINUTE SHE GENERALLY GAVE HERSELF VERY GOOD ADVICE THOUGH SHE VERY SELDOM FOLLOWED IT AND SOMETIMES SHE SCOLDED HERSELF SO SEVERELY AS TO BRING TEARS INTO HER EYES SHE ONCE REMEMBERED TRYING TO BOX HER OWN EARS FOR HAVING CHEATED HERSELF IN A GAME OF CROQUET SHE WAS PLAYING AGAINST HERSELF FOR THIS CURIOUS CHILD WAS FOND OF PRETENDING TO BE TWO PEOPLE BUT IT S NO USE NOW THOUGHT POOR ALICE TO PRETEND TO BE TWO PEOPLE WHY THERE S HARDLY ENOUGH OF ME TO MAKE ONE RESPECTABLE PERSON', 'SOON HER EYE FELL ON A LITTLE GLASS BOX THAT WAS LYING UNDER THE TABLE SHE OPENED IT AND FOUND IN IT A VERY SMALL CAKE ON WHICH THE WORDS EAT ME WERE BEAUTIFULLY MARKED IN WELL I EAT IT SAID ALICE AND IF IT MAKES ME GROW LARGER I CAN REACH THE KEY AND IF IT MAKES ME GROW SMALLER I CAN CREEP UNDER THE DOOR SO EITHER WAY I GET INTO THE GARDEN AND I DO CARE WHICH HAPPENS SHE ATE A LITTLE BIT AND SAID ANXIOUSLY TO HERSELF WHICH WAY WHICH WAY HOLDING HER HAND ON THE TOP OF HER HEAD TO FEEL WHICH WAY IT WAS GROWING AND SHE WAS QUITE SURPRISED TO FIND THAT SHE REMAINED THE SAME SIZE TO BE SURE THIS GENERALLY HAPPENS WHEN ONE EATS CAKE']\n"]}],"source":["TRANSCRIPTS = []\n","\n","for i in range(len(np.unique(data_csv.Segment))):\n","  words = \" \".join(data_csv[\"Word\"].loc[data_csv.Segment == i+1].values.tolist())\n","  words = \"\".join([i.upper() for i in words])\n","  TRANSCRIPTS.append(words)\n","\n","print(TRANSCRIPTS)"],"id":"_67FUEEpDRtH"},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1702335636493,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"OgFaQTwmCBPk"},"outputs":[],"source":["class EEGSpeechDatasetME(torch.utils.data.Dataset): # Memory efficient\n","    # Loades the data in get item to save RAM\n","\n","    def __init__(self, EEG_data, transcripts = TRANSCRIPTS):\n","\n","        self.VOCAB      = VOCAB\n","\n","        self.transcripts_files = TRANSCRIPTS\n","\n","        assert len(EEG_data) == len(transcripts)\n","\n","        length = len(mfcc_files) # TODO\n","\n","        self.EEG_files          =  EEG_data\n","        self.length             = len(self.EEG_files)\n","\n","    def __len__(self):\n","        # TODO\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","\n","        # Load the mfcc and transcripts from the mfcc and transcript paths created earlier\n","        EEG        = self.EEG_files[ind]  # TODO\n","\n","        transcript  = self.transcript_files[ind] # TODO\n","        transcript_mapped   = [SOS_TOKEN] + [VOCAB_MAP[i] for i in transcript] + [EOS_TOKEN]# TODO\n","        transcript_mapped =  np.array(transcript_mapped)\n","\n","        return torch.FloatTensor(EEG), torch.LongTensor(transcript_mapped)\n","\n","\n","    def collate_fn(self,batch):\n","\n","        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n","\n","        for x, y in batch:\n","            # Add the mfcc, transcripts and their lengths to the lists created above\n","            # TODO\n","            batch_x.append(x)\n","            batch_y.append(y)\n","            lengths_x.append(len(x))\n","            lengths_y.append(len(y))\n","\n","        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n","        batch_x_pad = pad_sequence(batch_x, batch_first=True, padding_value=PAD_TOKEN)# TODO\n","        batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=PAD_TOKEN)# TODO\n","\n","        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"],"id":"OgFaQTwmCBPk"},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9573,"status":"ok","timestamp":1702335646025,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"},"user_tz":300},"id":"bVNyet6EGnY9"},"outputs":[],"source":["import glob\n","from collections import Counter\n","\n","vhdr_files = glob.glob(\"drive/MyDrive/brennan data/*/*.vhdr\") # only use five? smaller dataset, more neurons??"],"id":"bVNyet6EGnY9"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFv--x8tGubG","executionInfo":{"status":"ok","timestamp":1702335900191,"user_tz":300,"elapsed":254207,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"be8b1db0-edd0-4b1a-af0b-7d35ad4d23fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading 1 of 49\n","Loading 2 of 49\n","Loading 3 of 49\n","Loading 4 of 49\n","Loading 5 of 49\n","Loading 6 of 49\n","Loading 7 of 49\n","Loading 8 of 49\n","Loading 9 of 49\n","Loading 10 of 49\n","Loading 11 of 49\n","Loading 12 of 49\n","Loading 13 of 49\n","Loading 14 of 49\n","Loading 15 of 49\n","Loading 16 of 49\n","Loading 17 of 49\n","Loading 18 of 49\n","Loading 19 of 49\n","Loading 20 of 49\n","Loading 21 of 49\n","Loading 22 of 49\n","Loading 23 of 49\n","Loading 24 of 49\n","Loading 25 of 49\n","Loading 26 of 49\n","Loading 27 of 49\n","Loading 28 of 49\n","Loading 29 of 49\n","Loading 30 of 49\n","Loading 31 of 49\n","Loading 32 of 49\n","Loading 33 of 49\n","Loading 34 of 49\n","Loading 35 of 49\n","Loading 36 of 49\n","Loading 37 of 49\n","Loading 38 of 49\n","Loading 39 of 49\n","Loading 40 of 49\n","Loading 41 of 49\n","Loading 42 of 49\n","Loading 43 of 49\n","Loading 44 of 49\n","Loading 45 of 49\n","Loading 46 of 49\n","Loading 47 of 49\n","Loading 48 of 49\n","Loading 49 of 49\n"]}],"source":["  eeg_data_list = []\n","  mne.set_log_level('WARNING')\n","\n","  for num, vhdr_file in enumerate(vhdr_files):\n","    print(f\"Loading {num + 1} of {len(vhdr_files)}\")\n","\n","    # Step 1: Load the file\n","    mne_raw = mne.io.read_raw_brainvision(vhdr_file, misc=['ECG', 'EMG', 'FootPad'],preload=True)\n","\n","    # A couple of the subjects have a different number of channels.\n","    # As someone who does EEG data research, I get it.\n","    # As someone doing this class project, I hate them.\n","    # Let's just ignore those people with different cap sizes to simplify things\n","    if mne_raw.get_data().shape[0] == 62:\n","\n","      # Step 2: Preprocess the filter\n","      # Note: This is very minimal, but we might not want to do too much more\n","\n","      # Bandpass filter 0.5 - 40 Hz\n","      mne_raw = mne_raw.filter(0.5, 40);\n","\n","      # Common Average Reference\n","      mne_raw.set_eeg_reference('average', projection=False);\n","\n","      # Resampling to 100 Hz is done AFTER segmentation into words to not lose\n","      # time resolution when cropping (see: .resample(100) below)\n","\n","      # Step 3: Segment into the 12 time-stamped segments\n","      # For some reason, some of the dataset first stimulus is called \"stimulus 1\"\n","      # and other times is new segment -.- . Also, calling events from annotations\n","      # can return the list in unsorted order, which will completely mess up the\n","      # whole labels to eeg alignment we have going on.\n","      # So,\n","      # mne_events, ev_id = mne.events_from_annotations(\n","      #           raw=mne_raw, event_id={\"Stimulus/{:d}\".format(i): i for i in range(0,13)})\n","\n","      mne_events, ev_id = mne.events_from_annotations(raw=mne_raw, event_id=Counter(mne_raw.annotations.description))\n","      # because strings are weird, this actually returns the events out of order.\n","      # so, let's sort them according to their timestamp. Easy peasy!\n","      mne_events = sorted(mne_events, key = lambda x: x[0])\n","\n","\n","      if len(mne_events) > 12:\n","        # Only keep last 12 if there was an extra starting stimulus,\n","        # Otherwise, consider the starting stimulus as start of sequence\n","        mne_events = mne_events[-12:]\n","\n","      # Crop each segment and save it seperately\n","      segments = [mne_raw.copy().crop(mne_events[i][0] * 1/mne_raw.info['sfreq'], mne_events[i+1][0] * 1/mne_raw.info['sfreq']).resample(100) for i in range(len(mne_events) - 1)]\n","      segments.append(mne_raw.copy().crop(mne_events[-1][0] * 1/mne_raw.info['sfreq'], None).resample(100))\n","      eeg_data_list.extend(segments)\n","      # print(eeg_data_list)"],"id":"eFv--x8tGubG"},{"cell_type":"code","execution_count":11,"metadata":{"id":"3b849d1b","executionInfo":{"status":"ok","timestamp":1702335900191,"user_tz":300,"elapsed":32,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# DATA_DIR    = \"drive/MyDrive/brennan data/S01\" # TODO: Path where you have downloaded the data\n","\n","# mne_raw = mne.io.read_raw_brainvision(os.path.join(DATA_DIR, \"S01.vhdr\"),\n","                                      # misc=['ECG', 'EMG', 'FootPad'],preload=True)"],"id":"3b849d1b"},{"cell_type":"code","execution_count":12,"metadata":{"id":"pHa6hIpfeEps","executionInfo":{"status":"ok","timestamp":1702335900192,"user_tz":300,"elapsed":23,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# # You'll notice in the dataframe that they've timestamped each word with regard to\n","# # the segmented file! That was very nice of them. Let's extract the word onset/offset\n","# import numpy as np\n","# segment_onset_offset = []\n","# for seg_val in np.unique(data_csv[\"Segment\"].values):\n","#   seg = data_csv.loc[data_csv.Segment == seg_val]\n","#   onsets = seg[\"onset\"].values\n","#   offsets = seg[\"offset\"].values\n","#   # Append to onset/offset times\n","#   segment_onset_offset.append(list(zip(onsets, offsets)))\n"],"id":"pHa6hIpfeEps"},{"cell_type":"code","execution_count":13,"metadata":{"id":"CCsWkoVZdEtM","executionInfo":{"status":"ok","timestamp":1702335900192,"user_tz":300,"elapsed":23,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# # Now its time to load in the EEG data\n","# # For this, we have to loop through all the subjects (49) and extract\n","# # each word based on the segment and timestamp\n","# import mne\n","# import glob\n","# from collections import Counter\n","\n","# if wannaSpendForeverMakingDataAgain:\n","\n","#   vhdr_files = glob.glob(\"drive/MyDrive/brennan data/*/*.vhdr\")\n","\n","#   vhdr_files\n","\n","#   eeg_data_list = []\n","#   labels_list = []\n","\n","\n","#   def data_sorter(data_string):\n","#       try:\n","#           return int(data_string.split(r\"/\")[-1])\n","#       except:\n","#           # handle division by zero error\n","#           # leave empty for now\n","#           return 0\n","\n","#   for vhdr_file in vhdr_files[26:]:\n","\n","#     # Step 1: Load the file\n","#     mne_raw = mne.io.read_raw_brainvision(vhdr_file, misc=['ECG', 'EMG', 'FootPad'],preload=True)\n","\n","\n","#     # A couple of the subjects have a different number of channels.\n","#     # As someone who does EEG data research, I get it.\n","#     # As someone doing this class project, I hate them.\n","#     # Let's just ignore those people with different cap sizes to simplify things\n","#     if mne_raw.get_data().shape[0] == 62:\n","\n","#       # Step 2: Preprocess the filter\n","#       # Note: This is very minimal, but we might not want to do too much more\n","\n","#       # Bandpass filter 0.5 - 40 Hz\n","#       mne_raw = mne_raw.filter(0.5, 40);\n","\n","#       # Common Average Reference\n","#       mne_raw.set_eeg_reference('average', projection=False);\n","\n","#       # Resampling to 100 Hz is done AFTER segmentation into words to not lose\n","#       # time resolution when cropping (see: .resample(100) below)\n","\n","#       # Step 3: Segment into the 12 time-stamped segments\n","#       # For some reason, some of the dataset first stimulus is called \"stimulus 1\"\n","#       # and other times is new segment -.- . Also, calling events from annotations\n","#       # can return the list in unsorted order, which will completely mess up the\n","#       # whole labels to eeg alignment we have going on.\n","#       # So,\n","#       # mne_events, ev_id = mne.events_from_annotations(\n","#       #           raw=mne_raw, event_id={\"Stimulus/{:d}\".format(i): i for i in range(0,13)})\n","\n","#       mne_events, ev_id = mne.events_from_annotations(raw=mne_raw, event_id=Counter(mne_raw.annotations.description))\n","#       # because strings are weird, this actually returns the events out of order.\n","#       # so, let's sort them according to their timestamp. Easy peasy!\n","#       mne_events = sorted(mne_events, key = lambda x: x[0])\n","\n","#       if len(mne_events) > 12:\n","#         # Only keep last 12 if there was an extra starting stimulus,\n","#         # Otherwise, consider the starting stimulus as start of sequence\n","#         mne_events = mne_events[-12:]\n","\n","#       # We're just going to crop at onset time. This is because the eeg timestamps sometimes overlap\n","#       # into the next segment. *eye roll*\n","#       segments = [mne_raw.copy().crop(mne_events[i][0] * 1/mne_raw.info['sfreq'], None) for i in range(len(mne_events))]\n","\n","#       # Step 4: Further break down each segment into the individual words\n","#       for i in range(len(segments)): # there should be 12extracted_sentences = [sentences[0].copy().crop(timestamps[i], timestamps[i+1]) for i in range(len(timestamps) - 1)]\n","#         #print(\"Segment: \", i)\n","#         extracted_words = [segments[i].copy().crop(segment_onset_offset[i][j][0], segment_onset_offset[i][j][1]).resample(100) for j in range(len(segment_onset_offset[i]))]\n","#         eeg_data_list.extend(extracted_words)\n","#         #print(eeg_data_list)\n"],"id":"CCsWkoVZdEtM"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCAWNc-vskEY","executionInfo":{"status":"ok","timestamp":1702335900192,"user_tz":300,"elapsed":23,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"13675282-cf62-4de0-ecdc-a530a884ccc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["564 564\n"]}],"source":["\n","#targets = data_csv['phonemes']\n","#phoneme_list = phoneme_list * len(vhdr_files) # Repeat the labels for each subject\n","\n","num_subj = len(eeg_data_list) // 12 # 12 sets per subject\n","targets = []\n","[targets.extend(TRANSCRIPTS) for _ in range(num_subj)]\n","print(len(eeg_data_list), len(targets))\n"],"id":"FCAWNc-vskEY"},{"cell_type":"code","execution_count":15,"metadata":{"id":"z5qo3ZKWosiW","executionInfo":{"status":"ok","timestamp":1702335900525,"user_tz":300,"elapsed":355,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# split into training, testing, and validation set\n","import sklearn\n","from sklearn import model_selection\n","\n","train_targets, val_targets, train_eeg, val_eeg = sklearn.model_selection.train_test_split(targets, eeg_data_list, test_size=0.20, random_state=42) # 80/20 train-val/test split\n","# train_targets, val_targets, train_eeg, val_eeg = sklearn.model_selection.train_test_split(train_val_targets, train_val_eeg, test_size=0.20, random_state=42) # 80/20 train / val split"],"id":"z5qo3ZKWosiW"},{"cell_type":"code","execution_count":38,"metadata":{"id":"66391693","executionInfo":{"status":"ok","timestamp":1702338513167,"user_tz":300,"elapsed":278,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# import torch\n","\n","# class Dataset(torch.utils.data.Dataset):\n","\n","#     def __init__(self, input_data, # EEG data\n","#                  output_data = None, # phoneme data\n","#                  mode = 'Training',\n","#                  phonemes = PHONEMES,\n","#                  min_length = 0,\n","#                 ): # Feel free to add more arguments\n","\n","#         \"\"\"\n","#         Input:\n","#         input_data: Preprocessed EEG data of array of shape (N_samples, ), where each sample is shape (n_time, n_channels),\n","#                     where n_channels is a consistent number across all samples\n","#         output_data: phoneme data of read transcript shape (N_samples, ), where each sample is shape (n_phonemes)\n","\n","#         \"\"\"\n","#         assert mode in ['Training', 'Testing', 'Validation'], \"Error! Unrecognized Mode\"\n","\n","#         if mode != \"Testing\":\n","#           data_tuple = [(i, j) for i,j in zip(input_data, output_data) if i.shape[0] > min_length]\n","#           input_data = [i[0] for i in data_tuple]\n","#           output_data = [i[1] for i in data_tuple]\n","#           print(\"Input data len: \", len(input_data))\n","#           assert len(input_data) == len(output_data)\n","\n","#         else:\n","#           input_data = [i for i in input_data if i.shape[0] > min_length]\n","\n","\n","#         self.EEG = input_data\n","#         self.length = len(self.EEG)\n","#         self.mode = mode\n","\n","#         if mode != 'Testing':\n","#             self.transcripts = []\n","#             for transcript in output_data: #transcript is one word\n","#                 # print(transcript)\n","#                 transcript_list = []\n","#                 for phoneme in transcript:\n","#                   phoneme = ''.join([i for i in phoneme if not i.isdigit()])\n","#                   transcript_list.append(phonemes.index(phoneme))\n","\n","#                 self.transcripts.append(transcript_list)\n","#         #print(self.transcripts)\n","\n","#     def __len__(self):\n","#         return self.length\n","\n","#     def __getitem__(self, ind):\n","\n","#         frames = self.EEG[int(ind)]\n","#         frames = torch.FloatTensor(frames) # Convert to tensors\n","\n","#         if self.mode in [\"Training\", \"Validation\"]:\n","#             #labels = torch.tensor(self.transcripts, dtype=torch.long)\n","#             labels    = torch.tensor(self.transcripts[ind], dtype=torch.long)\n","#             return frames, labels\n","\n","#         else:\n","#             return frames\n","\n","#     def collate_fn(self, batch):\n","#         '''\n","#         TODO:\n","#         1.  Extract the features and labels from 'batch'\n","#         2.  We will additionally need to pad both features and labels,\n","#             look at pytorch's docs for pad_sequence\n","#         3.  This is a good place to perform transforms, if you so wish.\n","#             Performing them on batches will speed the process up a bit.\n","#         4.  Return batch of features, labels, lenghts of features,\n","#             and lengths of labels.\n","#         '''\n","#         # batch of input mfcc coefficients\n","#         if self.mode in [\"Training\", \"Validation\"]:\n","#             batch_EEG =  [x[0] for x in batch]\n","#             lengths_EEG = [len(x[0]) for x in batch]# TODO\n","#             batch_EEG_pad = pad_sequence(batch_EEG, batch_first=True, padding_value=0.0)\n","\n","#             #batch_transcript = [x[0][1] for x in batch] # TODO\n","#             batch_transcript = [x[1] for x in batch]\n","\n","#             lengths_transcript = [len(x[1]) for x in batch] # TODO\n","#             #print('lengths_transcript is ', lengths_transcript)\n","#             #batch_transcript =  torch.tensor(batch_transcript)\n","\n","\n","#             # Batch x Time x 62 channels\n","#             batch_transcript_pad = pad_sequence(batch_transcript,\n","#                                                 batch_first=True,\n","#                                                 padding_value=0.0) # TODO\n","\n","#             return batch_EEG_pad, batch_transcript_pad, torch.tensor(lengths_EEG), torch.tensor(lengths_transcript)\n","\n","#         else:\n","#             batch_EEG =  [x for x in batch]\n","#             lengths_EEG = [len(x) for x in batch]# TODO\n","#             batch_EEG_pad = pad_sequence(batch_EEG, batch_first=True, padding_value=0.0)\n","\n","#             return batch_EEG_pad, torch.tensor(batch_EEG)"],"id":"66391693"},{"cell_type":"code","execution_count":39,"metadata":{"id":"ilN4TvITtZaR","executionInfo":{"status":"ok","timestamp":1702338513410,"user_tz":300,"elapsed":21,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["class EEGSpeechDatasetME(torch.utils.data.Dataset): # Memory efficient\n","    # Loades the data in get item to save RAM\n","\n","    def __init__(self, EEG_data, transcripts):\n","\n","        self.VOCAB      = VOCAB\n","        self.transcript_files = transcripts\n","\n","        assert len(EEG_data) == len(transcripts)\n","\n","        self.EEG_files          =  EEG_data\n","        self.length             =  len(self.EEG_files)\n","\n","    def __len__(self):\n","        # TODO\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","\n","        # Load the mfcc and transcripts from the mfcc and transcript paths created earlier\n","        EEG        = self.EEG_files[ind]  # TODO\n","\n","        transcript  = self.transcript_files[ind] # TODO\n","        transcript_mapped   = [SOS_TOKEN] + [VOCAB_MAP[i] for i in transcript] + [EOS_TOKEN] # TODO\n","        transcript_mapped =  np.array(transcript_mapped)\n","\n","        return torch.FloatTensor(EEG), torch.LongTensor(transcript_mapped)\n","\n","\n","    def collate_fn(self,batch):\n","\n","        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n","\n","        for x, y in batch:\n","            # Add the mfcc, transcripts and their lengths to the lists created above\n","            # TODO\n","            batch_x.append(x)\n","            batch_y.append(y)\n","            lengths_x.append(len(x))\n","            lengths_y.append(len(y))\n","\n","        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n","        batch_x_pad = pad_sequence(batch_x, batch_first=True, padding_value=PAD_TOKEN)# TODO\n","        batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=PAD_TOKEN)# TODO\n","\n","        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"],"id":"ilN4TvITtZaR"},{"cell_type":"code","execution_count":40,"metadata":{"id":"5FtXgDgPxovg","executionInfo":{"status":"ok","timestamp":1702338513410,"user_tz":300,"elapsed":20,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# Silence the logs because otherwise its just too much\n","\n","mne.set_log_level(False)\n","\n","def bandpass_filter_and_stack(mne_raw, all = True):\n","  if not all:\n","    theta = mne_raw.copy().filter(4, 8).get_data().T\n","    alpha = mne_raw.copy().filter(8, 12).get_data().T\n","    beta = mne_raw.copy().filter(12, 20).get_data().T\n","    gamma = mne_raw.copy().filter(20, 40).get_data().T\n","\n","    stacked = np.stack([theta, alpha, beta, gamma], axis = 1)\n","    return stacked\n","  else:\n","    all = mne_raw.copy().filter(4, 8).get_data().T\n","    return all\n","\n"],"id":"5FtXgDgPxovg"},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6y8CvwfUWjn","executionInfo":{"status":"ok","timestamp":1702338513410,"user_tz":300,"elapsed":20,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"1c3196ad-9404-459b-ac87-a823bb1fb482"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["451"]},"metadata":{},"execution_count":41}],"source":["len(train_targets)"],"id":"x6y8CvwfUWjn"},{"cell_type":"code","execution_count":42,"metadata":{"id":"6857e274","executionInfo":{"status":"ok","timestamp":1702338542643,"user_tz":300,"elapsed":29252,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["train_data = EEGSpeechDatasetME(EEG_data = [bandpass_filter_and_stack(i) for i in train_eeg], # .get_data().T for i in range(0, 4)],\n","                     transcripts = train_targets)\n","\n","val_data = EEGSpeechDatasetME(EEG_data = [bandpass_filter_and_stack(i) for i in val_eeg], # [extracted_sentences[4].get_data().T],\n","                     transcripts = val_targets)\n","\n","\n","# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_data,\n","    num_workers = 2, # <--- probably want to increase this :)\n","    batch_size  = config[\"batch_size\"],\n","    pin_memory  = True,\n","    shuffle     = True,\n","    collate_fn  = train_data.collate_fn\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    dataset     = val_data,\n","    num_workers = 2,\n","    batch_size  = config[\"batch_size\"],\n","    pin_memory  = True,\n","    shuffle     = False,\n","    collate_fn  = train_data.collate_fn\n",")\n","\n","# test_loader = torch.utils.data.DataLoader(\n","#     dataset     = test_data,\n","#     num_workers = 2,\n","#     batch_size  = 2,  # not a lot of samples . . .\n","#     pin_memory  = True,\n","#     shuffle     = False,\n","#     collate_fn  = train_data.collate_fn\n","# )"],"id":"6857e274"},{"cell_type":"code","source":["train_data.EEG_files[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pX5pBj51oR2H","executionInfo":{"status":"ok","timestamp":1702338542644,"user_tz":300,"elapsed":31,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"2cee25d3-47b6-40fe-d865-2eca2ad0f7db"},"id":"pX5pBj51oR2H","execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5738, 62)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["if True: #set to true if you want to do this 30 minute step. did it once and it worked\n","  def verify_dataset(dataset, partition= 'train-clean-100'):\n","      print(\"Max mfcc length          : \", np.max([data[0].shape[0] for data in dataset]))\n","      print(\"Avg mfcc length          : \", np.mean([data[0].shape[0] for data in dataset]))\n","      print(\"Max transcript length    : \", np.max([data[1].shape[0] for data in dataset]))\n","      print(\"Max transcript length    : \", np.mean([data[1].shape[0] for data in dataset]))\n","\n","  verify_dataset(train_data)\n","  verify_dataset(val_data)\n","  dataset_max_len  = max(\n","      np.max([data[0].shape[0] for data in train_data]),\n","      np.max([data[0].shape[0] for data in val_data]),\n","  )\n","  print(\"\\nMax Length: \", dataset_max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYIa028Akqh1","executionInfo":{"status":"ok","timestamp":1702338545377,"user_tz":300,"elapsed":2754,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"81e467e9-4286-44c4-c96b-0ca8cf3627f2"},"id":"sYIa028Akqh1","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Max mfcc length          :  7005\n","Avg mfcc length          :  6068.569844789357\n","Max transcript length    :  1052\n","Max transcript length    :  868.19955654102\n","Max mfcc length          :  7005\n","Avg mfcc length          :  6099.380530973452\n","Max transcript length    :  1052\n","Max transcript length    :  873.0265486725664\n","\n","Max Length:  7005\n"]}]},{"cell_type":"code","execution_count":45,"metadata":{"id":"d7dd3bf5","executionInfo":{"status":"ok","timestamp":1702338545380,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# # sanity check\n","# print(\"Batch size: \", config[\"batch_size\"])\n","# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","\n","# for i, data in enumerate(train_loader):\n","#     x, y, lx, ly = data\n","#     print(x.shape, y.shape, lx.shape, ly.shape)\n","#     print(y)\n","#     break"],"id":"d7dd3bf5"},{"cell_type":"code","execution_count":46,"metadata":{"id":"0f6bab63","executionInfo":{"status":"ok","timestamp":1702338545381,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["class PermuteBlock(torch.nn.Module):\n","    def forward(self, x):\n","        return x.transpose(1, 2)\n","\n","def plot_attention(attention):\n","    # Function for plotting attention\n","    # You need to get a diagonal plot\n","    plt.clf()\n","    seaborn.heatmap(attention, cmap='GnBu')\n","    plt.show()\n","\n","def save_model(model, optimizer, scheduler, tf_scheduler, metric, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         'scheduler_state_dict'     : scheduler.state_dict(),\n","         'tf_rate'                  : tf_rate,\n","         metric[0]                  : metric[1],\n","         'epoch'                    : epoch},\n","         path\n","    )\n","\n","def load_model(best_path, epoch_path, model, mode= 'best', metric= 'valid_acc', optimizer= None, scheduler= None, tf_scheduler= None):\n","\n","\n","    if mode == 'best':\n","        checkpoint  = torch.load(best_path)\n","        print(\"Loading best checkpoint: \", checkpoint[metric])\n","    else:\n","        checkpoint  = torch.load(epoch_path)\n","        print(\"Loading epoch checkpoint: \", checkpoint[metric])\n","\n","    model.load_state_dict(checkpoint['model_state_dict'], strict= False)\n","\n","    if optimizer != None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        #optimizer.param_groups[0]['lr'] = 1.5e-3\n","        optimizer.param_groups[0]['weight_decay'] = 1e-5\n","    if scheduler != None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    if tf_scheduler != None:\n","        tf_scheduler    = checkpoint['tf_scheduler']\n","\n","    epoch   = checkpoint['epoch']\n","    metric  = torch.load(best_path)[metric]\n","\n","    return [model, optimizer, scheduler, tf_scheduler, epoch, metric]\n","\n","class TimeElapsed():\n","    def __init__(self):\n","        self.start  = -1\n","\n","    def time_elapsed(self):\n","        if self.start == -1:\n","            self.start = time.time()\n","        else:\n","            end = time.time() - self.start\n","            hrs, rem    = divmod(end, 3600)\n","            min, sec    = divmod(rem, 60)\n","            min         = min + 60*hrs\n","            print(\"Time Elapsed: {:0>2}:{:02}\".format(int(min),int(sec)))\n","            self.start  = -1"],"id":"0f6bab63"},{"cell_type":"code","execution_count":47,"metadata":{"id":"2O9XIo_nAvRe","executionInfo":{"status":"ok","timestamp":1702338545381,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"621ddd47-d939-4813-b96c-7f67a4640519"},"outputs":[{"output_type":"stream","name":"stdout","text":["TransformerEncoder(\n","  (KW): Linear(in_features=256, out_features=256, bias=True)\n","  (VW): Linear(in_features=256, out_features=256, bias=True)\n","  (QW): Linear(in_features=256, out_features=256, bias=True)\n","  (permute): PermuteBlock()\n","  (attention): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","  )\n","  (bn1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","  (bn2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n","  (MLP): Linear(in_features=256, out_features=256, bias=True)\n","  (activ): ReLU()\n","  (MLP2): Linear(in_features=256, out_features=256, bias=True)\n",")\n","=========================================================\n","        Kernel Shape     Output Shape   Params Mult-Adds\n","Layer                                                   \n","0_bn1          [256]  [128, 176, 256]    512.0     256.0\n","1_MLP     [256, 256]  [128, 176, 256]  65.792k   65.536k\n","2_activ            -  [128, 176, 256]        -         -\n","3_MLP2    [256, 256]  [128, 176, 256]  65.792k   65.536k\n","4_bn2          [256]  [128, 176, 256]    512.0     256.0\n","---------------------------------------------------------\n","                        Totals\n","Total params          132.608k\n","Trainable params      132.608k\n","Non-trainable params       0.0\n","Mult-Adds             131.584k\n","=========================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]}],"source":["import math\n","\n","class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, projection_size, max_seq_len= 3500, dropout=0.2):\n","        super().__init__()\n","        # Read the Attention Is All You Need paper to learn how to code code the positional encoding\n","\n","        # TODO\n","        self.dropout = nn.Dropout(dropout)\n","        K = max_seq_len     #???\n","        d = projection_size #???\n","\n","        #self.P = torch.zeros((K,d)).to(DEVICE) #K is position of object in sequence, d is dimension of output embedding\n","\n","        #for t in range(0, max_seq_len):\n","        #  for i in range(0, int(d/2)):\n","        #    denominator = 10000 ** (2*i/d)\n","        #    self.P[t,2*i] = math.sin(t/denominator)\n","        #    self.P[t,2*i+1] = math.cos(t/denominator)\n","        #self.P = self.P.unsqueeze(0)\n","\n","        self.P = torch.zeros(max_seq_len, projection_size).to(DEVICE)\n","        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, projection_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / projection_size))\n","        self.P[:, 0::2] = torch.sin(position * div_term)\n","        self.P[:, 1::2] = torch.cos(position * div_term)\n","        self.P = self.P.unsqueeze(0)\n","\n","\n","    def forward(self, x):\n","        #x = x + self.P[:, :x.shape[0],:]\n","        #print('forward')\n","        #print(x.shape)\n","        #print(self.P.shape)\n","        x = x + self.P[:, :x.size(1), :]\n","        #print(x.shape)\n","        x = self.dropout(x)\n","        return x\n","\n","class TransformerEncoder(torch.nn.Module):\n","    def __init__(self, projection_size, num_heads, dropout= 0.2):\n","        super().__init__()\n","\n","        # create the key, query and value weights\n","        #print(projection_size)\n","        self.KW         = nn.Linear(projection_size, projection_size) # TODO\n","        self.VW         = nn.Linear(projection_size, projection_size) # TODO\n","        self.QW         = nn.Linear(projection_size, projection_size) # TODO\n","\n","        self.permute    = PermuteBlock()\n","        # Compute multihead attention. You are free to use the version provided by pytorch\n","        self.attention  = torch.nn.MultiheadAttention(embed_dim=projection_size, num_heads=num_heads, dropout=dropout, batch_first=True) # TODO\n","\n","        #ohhh bn means batch norm\n","        self.bn1        = nn.LayerNorm(projection_size, eps=1e-6) # TODO\n","\n","        self.bn2        = nn.LayerNorm(projection_size, eps=1e-6) # TODO\n","\n","        # Feed forward neural network\n","        self.MLP        = nn.Linear(projection_size,projection_size) #???\n","        self.activ      = nn.ReLU()\n","        self.MLP2        = nn.Linear(projection_size,projection_size)\n","\n","    def forward(self, x):\n","        # compute the key, query and value\n","        #print(self.projection_size.shape)\n","        #print(x.shape)\n","        #if len(x.shape) > 2:\n","          #x = torch.permute(x, (1,2,0))\n","        #print('transformer1')\n","        x = x.to(DEVICE)\n","        #print(x.shape)\n","        key     = self.KW.forward(x) # TODO\n","        #print('transformer2')\n","        value   = self.VW.forward(x) # TODO\n","        query   = self.QW.forward(x) # TODO\n","        #print('transformer2')\n","\n","        # compute the output of the attention module\n","        out1    = self.attention.forward(query, key, value, need_weights=False) # TODO\n","        #print('transformer3')\n","        # Create a residual connection between the input and the output of the attention module\n","        #print('hi ', out1.shape)\n","        #print(x.shape)\n","        #x = torch.permute(x,(2,0,1))\n","        out1    = out1[0] + x\n","        # Apply batch norm to out1\n","        out1    = self.bn1(out1)\n","\n","        # Apply the output of the feed forward network\n","        out2    = self.MLP(out1) # TODO\n","        out2    = self.activ(out2)\n","        out2    = self.MLP2(out2)\n","        # Apply a residual connection between the input and output of the  FFN\n","        out2    = out2 + out1 # TODO\n","        # Apply batch norm to the output\n","        out3    = self.bn2(out2) # TODO\n","\n","        return out3\n","\n","model   = TransformerEncoder(\n","    projection_size  = 256,\n","    num_heads=4\n",").to(DEVICE)\n","\n","\n","print(model)\n","\n","x_sample    = torch.rand(128, 176, 256)\n","torchsummaryX.summary(model, x_sample.to(DEVICE))\n","del x_sample"],"id":"2O9XIo_nAvRe"},{"cell_type":"code","execution_count":48,"metadata":{"id":"HGBRStp3AyLc","executionInfo":{"status":"ok","timestamp":1702338545381,"user_tz":300,"elapsed":33,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["class TransformerListener(torch.nn.Module):\n","\n","    def __init__(self,\n","                 input_size              = 62,\n","                 base_lstm_layers        = 1,\n","                 pblstm_layers           = 1,\n","                 listener_hidden_size    = 512,\n","                 n_heads                 = 8,\n","                 tf_blocks               = 1):\n","        super().__init__()\n","\n","        # create an lstm layer\n","        self.base_lstm      = nn.LSTM(input_size = input_size, hidden_size = listener_hidden_size, num_layers = base_lstm_layers, bidirectional=True, batch_first=True)\n","\n","\n","        # create a sequence of Conv1d layers\n","        self.kernel_size = 3\n","        self.embedding      = torch.nn.Conv1d(1024, 512, self.kernel_size, 3)\n","        #elf.embedding      = torch.nn.Embedding(num_embeddings= len(VOCAB), embedding_dim = 1024, padding_idx= PAD_TOKEN)\n","\n","        # compute the position encoding\n","        self.positional_encoding    = PositionalEncoding(projection_size=512) # TODO\n","\n","        # create a sequence of transformer blocks\n","        #self.transformer_encoder    = torch.nn.Sequential(nn.Linear(128, 128), nn.Linear(128, 128))\n","        self.transformer_encoder = TransformerEncoder(projection_size=512, num_heads=n_heads)\n","        #for i in range(tf_blocks):\n","        #    pass\n","        #    # TODO\n","\n","    def forward(self, x, x_len):\n","\n","        # pack the inputs before passing them to the LSTm\n","        #print(x.shape)\n","        #print(x_len.shape)\n","        #print('listenerforward1')\n","        x_packed                = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False) # TODO\n","        # Pass the packed sequence through the lstm\n","        #print('hi ', x_packed[0].shape)\n","        #print('listenerforward2')\n","        lstm_out, _             = self.base_lstm(x_packed)\n","        #print('hi')\n","        # Unpack the output of the lstm\n","        #print('listenerforward3')\n","        output, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n","\n","        # Pass the output through the embedding\n","        #print('listenerforward4')\n","        output = torch.permute(output,(0,2,1))\n","        output = self.embedding(output)\n","        output = torch.permute(output,(0,2,1))\n","        # calculate the new output length\n","\n","        output_lengths          = 1 + (output_lengths-self.kernel_size)//2\n","\n","        # calculate the position encoding\n","        #print('listenerforward5')\n","        output  = self.positional_encoding(output)\n","        #print('encoding')\n","        #print(pos_enc.shape)\n","        # Pass the output of the positional encoding through the transformer encoder\n","        #print('listenerforward6')\n","        #output  = self.transformer_encoder(output)\n","        #print('listenerforward7')\n","        #print('listener')\n","        #print(x.shape)\n","        #print(x_len.shape)\n","        #print(output.shape)\n","        #print(output_lengths.shape)\n","\n","        return output, output_lengths"],"id":"HGBRStp3AyLc"},{"cell_type":"code","execution_count":49,"metadata":{"id":"dwp7ZF3rA0g6","executionInfo":{"status":"ok","timestamp":1702338545384,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["class Attention(torch.nn.Module):\n","  def __init__(self, listener_hidden_size=512, speller_hidden_size=256, projection_size=256):\n","    super().__init__()\n","    self.WK = nn.Linear(listener_hidden_size, projection_size)\n","    self.WV = nn.Linear(listener_hidden_size, projection_size)\n","    self.WQ = nn.Linear(speller_hidden_size, projection_size)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","  def set_key_value(self, encoder_outputs):\n","    #for b in range(0, encoder_outputs.shape[0]):\n","    self.key = self.WK.forward(encoder_outputs)\n","    self.value = self.WV.forward(encoder_outputs)\n","\n","\n","  def compute_context(self, decoder_context):\n","    #print('YO', decoder_context.shape)\n","    #print(self.key.shape)\n","    #attention_weights = torch.zeros(config['batch_size'], decoder_context.shape[0], self.key.shape[1])\n","    #context = torch.zeros(config['batch_size'], self.value.shape[1])\n","    query_length = len(decoder_context[0,:])\n","\n","    query = self.WQ(decoder_context)\n","    self.query = query\n","    #print('query key weights ', query.shape)\n","    #print(self.key.shape)\n","    #raw_weights = (1/math.sqrt(query_length)) * torch.bmm(query.unsqueeze(1), torch.transpose(self.key, 1, 2))\n","    #print(self.key.shape)\n","    #print(query.shape)\n","    raw_weights = (1/math.sqrt(query_length)) * torch.bmm(self.key, query.unsqueeze(2))\n","    raw_weights = raw_weights.squeeze(2)\n","    #print(raw_weights.shape)\n","    attention_weights = self.softmax(raw_weights)\n","    #print(attention_weights.shape)\n","    #print('context ', context.shape)\n","    #print(self.value.shape)\n","    context = torch.bmm(attention_weights.unsqueeze(1), self.value).squeeze(1)\n","\n","    return context, attention_weights"],"id":"dwp7ZF3rA0g6"},{"cell_type":"code","execution_count":50,"metadata":{"id":"Y33UTg_tA3Vu","executionInfo":{"status":"ok","timestamp":1702338545384,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["from sre_constants import JUMP\n","class Speller(torch.nn.Module):\n","\n","  # Refer to your HW4P1 implementation for help with setting up the language model.\n","  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n","\n","  def __init__(self, attender:Attention):\n","    super(). __init__()\n","\n","    self.attend = attender # Attention object in speller\n","    #self.max_timesteps = 600 # Max transcript length\n","    self.max_timesteps = 1100\n","\n","    #self.embedding = torch.nn.Conv1d(28, 64, 3, 2) # Embedding layer to convert token to latent space\n","    self.embedding = nn.Embedding(num_embeddings= len(VOCAB), embedding_dim = 512, padding_idx= PAD_TOKEN) #embed to lower dimension ??\n","    self.positional_encoding    = PositionalEncoding(projection_size=512)\n","\n","    self.lstm_output_chan = 256\n","    self.lstm_cells = nn.ModuleList([nn.LSTMCell(768, 512),  nn.LSTMCell(512, self.lstm_output_chan)]) # Create a sequence of LSTM Cells\n","    self.lstm_cells = nn.Sequential(*self.lstm_cells)\n","    #self.lstm_cells = nn.LSTMCell(71,self.lstm_output_chan)\n","    drop_layers    = [nn.Dropout(p=0.2)] * len(self.lstm_cells)\n","    self.dropout_lays = nn.ModuleList(drop_layers)\n","    # For CDN (Feel free to change)\n","    self.output_to_char = nn.Linear(512, len(VOCAB)) # Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n","    #self.activation = nn.Tanh() # Check which activation is suggested\n","    #self.char_prob = nn.Softmax(dim=1) # Linear layer to convert hidden space back to logits for token classification\n","    self.output_to_char.weight = self.embedding.weight # Weight tying (From embedding layer)\n","\n","\n","  def lstm_step(self, input_word, hidden_state):\n","    #print(lstm_input)\n","    #print(hidden_state)\n","    #for j in range(len(self.lstm_cells)):\n","    #    # Feed the input through each LSTM Cell\n","    #    print(lstm_input.dim())\n","    #    hidden_state[j] = self.lstm_cells.forward(lstm_input)\n","    #    lstm_input = hidden_state[j]\n","    #print(lstm_input.shape)\n","    #print(hidden_state.shape)\n","    #print(input_word.shape)\n","    for j in range(len(self.lstm_cells)):\n","        #print(j)\n","        #print(\"hidden: \", hidden_state[j] if hidden_state[j] is not None else \"None\")\n","        if hidden_state[j] is None:\n","            hidden_state[j] = self.lstm_cells[j](input_word, hidden_state[j])\n","        else:\n","            hidden_state[j] = self.lstm_cells[j](input_word, hidden_state[j])\n","        #print('check')\n","        input_word = hidden_state[j][0]\n","        input_word = self.dropout_lays[j](input_word)\n","        hidden_state[j] = (input_word, hidden_state[j][1])\n","    return input_word, hidden_state # What information does forward() need?\n","\n","  def CDN(self,CDN_input):\n","    # Make the CDN here, you can add the output-to-char\n","    out = self.output_to_char.forward(CDN_input)\n","    #out = self.activation.forward(out)\n","    #print('look here', out.shape)\n","    #out = self.char_prob.forward(out)\n","    #print(out.shape)\n","    return out\n","\n","  def forward (self, y=None, teacher_forcing_ratio=1):\n","\n","    attn_context = torch.zeros((config['batch_size'],256)) # initial context tensor for time t = 0\n","    output_symbol = torch.tensor(SOS_TOKEN) # Set it to SOS for time t = 0\n","    output_symbol = output_symbol.repeat((config['batch_size']))\n","    raw_outputs = []\n","    attention_plot = []\n","\n","    if y is None:\n","        timesteps = self.max_timesteps\n","        teacher_forcing_ratio = 0 #Why does it become zero? we don't have label y\n","\n","    else:\n","        timesteps = len(y[0,:]) # How many timesteps are we predicting for?\n","        label = self.embedding(y)\n","\n","    hidden_states_list = [None]*len(self.lstm_cells)\n","    #hidden_states_list = torch.zeros(config['batch_size'],self.lstm_output_chan)\n","    # Initialize your hidden_states list here similar to HW4P1\n","\n","    for t in range(timesteps):\n","        p = np.random.random_sample() # generate a probability p between 0 and 1\n","\n","        if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n","            char_embed = label[:,t-1,:] # Take from y, else draw from probability distribution\n","        else:\n","            char_embed = self.embedding(output_symbol.to(DEVICE)) #output symbol remains as output symbol from before\n","\n","        char_embed = self.positional_encoding(char_embed.unsqueeze(1)).squeeze(1)\n","        #print(output_symbol)\n","        #print(output_symbol.shape)\n","        #print(y.shape)\n","        output_symbol = output_symbol.to(DEVICE)\n","        #char_embed = self.embedding(output_symbol) # Embed the character symbol\n","\n","        #print(y[t-1])\n","        # Concatenate the character embedding and context from attention, as shown in the diagram\n","        #print('embedding...')\n","        #print(char_embed.shape)\n","        #print(attn_context.shape)\n","        #print('char ', char_embed.shape)\n","        #print(attn_context.shape)\n","        lstm_input = torch.cat((char_embed, attn_context.to(DEVICE)),1)\n","\n","        input_word, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n","        # What should we retrieve from forward_step to prepare for the next timestep?\n","        attn_context, attn_weights = self.attend.compute_context(hidden_states_list[-1][0]) # Feed the resulting hidden state into attention\n","\n","        #print(self.attend.query.shape)\n","        #print(attn_context.shape)\n","        cdn_input = torch.cat((self.attend.query, attn_context.to(DEVICE)),1) # TODO: You need to concatenate the context from the attention module with the LSTM output hidden state, as shown in the diagram\n","\n","        raw_pred = self.CDN(cdn_input) # call CDN with cdn_input\n","\n","        # Generate a prediction for this timestep and collect it in output_symbols\n","        #print(raw_pred)\n","        #print(raw_pred.shape)\n","        #print(raw_pred)\n","        #print(raw_pred.shape)\n","        output_symbol = torch.argmax(raw_pred,1) # Draw correctly from raw_pred\n","\n","        raw_outputs.append(raw_pred) # for loss calculation\n","        attention_plot.append(attn_weights) # for plotting attention plot\n","\n","\n","    attention_plot = torch.stack(attention_plot, dim=1)\n","    raw_outputs = torch.stack(raw_outputs, dim=1)\n","\n","    return raw_outputs, attention_plot"],"id":"Y33UTg_tA3Vu"},{"cell_type":"code","execution_count":51,"metadata":{"id":"3f77f5cf","executionInfo":{"status":"ok","timestamp":1702338545384,"user_tz":300,"elapsed":35,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["class ASRModel(torch.nn.Module):\n","  def __init__(self): # add parameters\n","    super().__init__()\n","\n","    # Pass the right parameters here\n","    self.listener = TransformerListener()\n","    self.attend = Attention()\n","    self.speller = Speller(self.attend)\n","\n","  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n","    # Encode speech features\n","    encoder_outputs, lx = self.listener(x,lx)\n","\n","    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n","    # Set keys and values using the encoder outputs\n","    #print(encoder_outputs.shape)\n","    self.attend.set_key_value(encoder_outputs)\n","\n","    # Decode text with the speller using context from the attention\n","    raw_outputs, attention_plots = self.speller(y=y,teacher_forcing_ratio=teacher_forcing_ratio)\n","\n","    return raw_outputs, attention_plots"],"id":"3f77f5cf"},{"cell_type":"code","execution_count":52,"metadata":{"id":"7c78fb71","executionInfo":{"status":"ok","timestamp":1702338546049,"user_tz":300,"elapsed":700,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"87cea72c-d40e-4a67-9c7b-146a506b1bdd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1348"]},"metadata":{},"execution_count":52}],"source":["torch.cuda.empty_cache()\n","gc.collect()"],"id":"7c78fb71"},{"cell_type":"code","source":["model = ASRModel(\n","\n","    # Initialize your model\n",")\n","\n","model = model.to(DEVICE)\n","print(model)\n","\n","x_sample    = torch.rand(36, 1700, 62)\n","lx_sample   = torch.randint(1, 500, (36,))\n","torchsummaryX.summary(model, x_sample.to(DEVICE), lx_sample)\n","del x_sample\n","del lx_sample"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqurZot_Ej3T","executionInfo":{"status":"ok","timestamp":1702338559686,"user_tz":300,"elapsed":13660,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"8418c3d1-c73b-4ec9-9eca-8078f669150b"},"id":"YqurZot_Ej3T","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["ASRModel(\n","  (listener): TransformerListener(\n","    (base_lstm): LSTM(62, 512, batch_first=True, bidirectional=True)\n","    (embedding): Conv1d(1024, 512, kernel_size=(3,), stride=(3,))\n","    (positional_encoding): PositionalEncoding(\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","    (transformer_encoder): TransformerEncoder(\n","      (KW): Linear(in_features=512, out_features=512, bias=True)\n","      (VW): Linear(in_features=512, out_features=512, bias=True)\n","      (QW): Linear(in_features=512, out_features=512, bias=True)\n","      (permute): PermuteBlock()\n","      (attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","      )\n","      (bn1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (bn2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (MLP): Linear(in_features=512, out_features=512, bias=True)\n","      (activ): ReLU()\n","      (MLP2): Linear(in_features=512, out_features=512, bias=True)\n","    )\n","  )\n","  (attend): Attention(\n","    (WK): Linear(in_features=512, out_features=256, bias=True)\n","    (WV): Linear(in_features=512, out_features=256, bias=True)\n","    (WQ): Linear(in_features=256, out_features=256, bias=True)\n","    (softmax): Softmax(dim=1)\n","  )\n","  (speller): Speller(\n","    (attend): Attention(\n","      (WK): Linear(in_features=512, out_features=256, bias=True)\n","      (WV): Linear(in_features=512, out_features=256, bias=True)\n","      (WQ): Linear(in_features=256, out_features=256, bias=True)\n","      (softmax): Softmax(dim=1)\n","    )\n","    (embedding): Embedding(31, 512, padding_idx=0)\n","    (positional_encoding): PositionalEncoding(\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","    (lstm_cells): Sequential(\n","      (0): LSTMCell(768, 512)\n","      (1): LSTMCell(512, 256)\n","    )\n","    (dropout_lays): ModuleList(\n","      (0): Dropout(p=0.2, inplace=False)\n","      (1): Dropout(p=0.2, inplace=False)\n","    )\n","    (output_to_char): Linear(in_features=512, out_features=31, bias=True)\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]},{"output_type":"stream","name":"stdout","text":["=========================================================================================================\n","                                                  Kernel Shape  \\\n","Layer                                                            \n","0_listener.LSTM_base_lstm                                    -   \n","1_listener.Conv1d_embedding                     [1024, 512, 3]   \n","2_listener.positional_encoding.Dropout_dropout               -   \n","3_speller.Embedding_embedding                        [512, 31]   \n","4_speller.positional_encoding.Dropout_dropout                -   \n","...                                                        ...   \n","10998_speller.dropout_lays.Dropout_0                         -   \n","10999_speller.attend.Linear_WQ                      [256, 256]   \n","11000_speller.attend.Linear_WQ                      [256, 256]   \n","11001_speller.attend.Softmax_softmax                         -   \n","11002_speller.attend.Softmax_softmax                         -   \n","\n","                                                  Output Shape     Params  \\\n","Layer                                                                       \n","0_listener.LSTM_base_lstm                         [8289, 1024]  2.359296M   \n","1_listener.Conv1d_embedding                     [36, 512, 166]  1.573376M   \n","2_listener.positional_encoding.Dropout_dropout  [36, 166, 512]          -   \n","3_speller.Embedding_embedding                        [36, 512]    15.872k   \n","4_speller.positional_encoding.Dropout_dropout     [36, 1, 512]          -   \n","...                                                        ...        ...   \n","10998_speller.dropout_lays.Dropout_0                 [36, 256]          -   \n","10999_speller.attend.Linear_WQ                       [36, 256]          -   \n","11000_speller.attend.Linear_WQ                       [36, 256]          -   \n","11001_speller.attend.Softmax_softmax                 [36, 166]          -   \n","11002_speller.attend.Softmax_softmax                 [36, 166]          -   \n","\n","                                                  Mult-Adds  \n","Layer                                                        \n","0_listener.LSTM_base_lstm                         2.351104M  \n","1_listener.Conv1d_embedding                     261.095424M  \n","2_listener.positional_encoding.Dropout_dropout            -  \n","3_speller.Embedding_embedding                       15.872k  \n","4_speller.positional_encoding.Dropout_dropout             -  \n","...                                                     ...  \n","10998_speller.dropout_lays.Dropout_0                      -  \n","10999_speller.attend.Linear_WQ                      65.536k  \n","11000_speller.attend.Linear_WQ                      65.536k  \n","11001_speller.attend.Softmax_softmax                      -  \n","11002_speller.attend.Softmax_softmax                      -  \n","\n","[11003 rows x 4 columns]\n","---------------------------------------------------------------------------------------------------------\n","                            Totals\n","Total params             7.428352M\n","Trainable params         7.428352M\n","Non-trainable params           0.0\n","Mult-Adds             4.173744128G\n","=========================================================================================================\n"]}]},{"cell_type":"code","source":["optimizer   = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=True, weight_decay=0) # TODO\n","\n","criterion   = torch.nn.CrossEntropyLoss(ignore_index = 0) # TODO\n","\n","scaler      = torch.cuda.amp.GradScaler()\n","\n","scheduler   = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.95, last_epoch=-1)"],"metadata":{"id":"6II8t2YeEnPb","executionInfo":{"status":"ok","timestamp":1702338559686,"user_tz":300,"elapsed":54,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"id":"6II8t2YeEnPb","execution_count":54,"outputs":[]},{"cell_type":"code","execution_count":55,"metadata":{"id":"6eO_ljJ_tFyQ","executionInfo":{"status":"ok","timestamp":1702338559687,"user_tz":300,"elapsed":54,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"outputs":[],"source":["# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n","def indices_to_chars(indices, vocab):\n","    tokens = []\n","    for i in indices: # This loops through all the indices\n","        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n","            continue\n","        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n","            break\n","        else:\n","            tokens.append(vocab[i])\n","    return tokens\n","\n","# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n","def calc_edit_distance(predictions, y, y_len, vocab= VOCAB, print_example= False, batch_num = -1):\n","\n","    dist                = 0\n","    batch_size, seq_len = predictions.shape\n","\n","    for batch_idx in range(batch_size):\n","\n","        y_sliced    = indices_to_chars(y[batch_idx,0:y_len[batch_idx]], vocab)\n","        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n","\n","        # Strings - When you are using characters from the AudioDataset\n","        y_string    = ''.join(y_sliced)\n","        pred_string = ''.join(pred_sliced)\n","\n","        #dist        += Levenshtein.distance(pred_string, y_string)\n","        # Comment the above abd uncomment below for toy dataset\n","        dist      += Levenshtein.distance(y_sliced, pred_sliced)\n","\n","    if (print_example) and (batch_num == 0):\n","        # Print y_sliced and pred_sliced if you are using the toy dataset\n","        print(\"\\nGround Truth : \", y_string)\n","        print(\"Prediction   : \", pred_string)\n","\n","    dist    /= batch_size\n","    return dist\n","\n","\n","def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n","\n","    model.train()\n","    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    running_loss        = 0.0\n","    running_perplexity  = 0.0\n","\n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","        if i == 12:\n","          continue\n","        optimizer.zero_grad()\n","\n","        x, y, lx, ly = x.to(device), y.to(device), lx, ly\n","\n","        with torch.cuda.amp.autocast():\n","\n","            raw_predictions, attention_plot = model(x, lx, y= y, teacher_forcing_ratio = teacher_forcing_rate)\n","\n","            # Predictions are of Shape (batch_size, timesteps, vocab_size).\n","            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n","            # So in total, you have batch_size*timesteps amount of characters.\n","            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n","            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n","            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with\n","            # your implementation\n","\n","            # loss function has been set to ingnore padding index\n","            # print(\"Pred shape: \", raw_predictions.shape, raw_predictions.data.shape)\n","            # print(\"Y shape: \", y.shape, y.data.shape)\n","\n","            loss        =  criterion(raw_predictions.view(-1, raw_predictions.shape[2]), y.view(-1,)) # TODO: Cross Entropy Loss\n","\n","            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n","            running_loss        += loss.item()\n","            running_perplexity  += perplexity.item()\n","\n","        # Backward on the masked loss\n","        scaler.scale(loss).backward()\n","        # loss.backward()\n","\n","        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n","        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n","\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(running_loss/(i+1)),\n","            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n","            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n","            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    running_loss /= len(dataloader)\n","    running_perplexity /= len(dataloader)\n","    batch_bar.close()\n","\n","    return running_loss, running_perplexity, attention_plot\n","\n","def validate(model, dataloader):\n","\n","    model.eval()\n","\n","    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","\n","    running_lev_dist = 0.0\n","\n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","        if i == 3:\n","          continue\n","        x, y, lx, ly = x.to(device), y.to(device), lx, ly\n","\n","        with torch.inference_mode():\n","            raw_predictions, attentions = model(x, lx, y = None)\n","\n","        # Greedy Decoding\n","        greedy_predictions   = torch.argmax(raw_predictions, dim = 2) # TODO: How do you get the most likely character from each distribution in the batch?\n","\n","        # Calculate Levenshtein Distance\n","        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = True,\n","                                                  batch_num = i) # You can use print_example = True for one specific index i in your batches if you want\n","\n","        batch_bar.set_postfix(\n","            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    running_lev_dist /= len(dataloader)\n","\n","    return running_lev_dist"],"id":"6eO_ljJ_tFyQ"},{"cell_type":"markdown","metadata":{"id":"c28ba3ca"},"source":["EEG to phonemes\n","\n","input EEG --> output phoneme\n","\n","EEG = (12 * N_subjects,) varying length\n","\n","\"labels\" = list (12 * N_subjects,) varying length\n","\n","\n","Follow HW3P2 logic\n","\n","Padpacked sequence whatever dataloader"],"id":"c28ba3ca"},{"cell_type":"code","execution_count":56,"metadata":{"id":"RBeBTJj-sghv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702338559687,"user_tz":300,"elapsed":54,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"1d95a5ac-c2c3-47bc-aebf-d64f813b2de1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":56}],"source":["# Login to Wandb\n","import wandb\n","wandb.login(key=\"eb0a91ea57d10554fba242aafcda65f03bb4ccc3\")\n",""],"id":"RBeBTJj-sghv"},{"cell_type":"code","execution_count":57,"metadata":{"id":"Aw1BG-EzshMj","colab":{"base_uri":"https://localhost:8080/","height":373,"referenced_widgets":["0fe76819e05343d0a12cc5eb9a5169d2","5ed131cc93454ae899962a15af5889fd","1bbec8042c5c4eef874465679e6900cf","5e05f309b6014433b7975a2fd6daac57","cd45fd8585b3462ba8643ad0d0033300","90fa39a042bc4cbda1dc1f7fac70060c","ec27ce4efd414a89bdd831bcdf795dcf","a4ddee77ad26465d846b80b363005d5a"]},"executionInfo":{"status":"ok","timestamp":1702338566892,"user_tz":300,"elapsed":7256,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}},"outputId":"7e954a52-a96e-4a14-955e-c7b3b5673f72"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:7xo0wsay) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe76819e05343d0a12cc5eb9a5169d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_perplex</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_dist</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>2.61488</td></tr><tr><td>train_perplex</td><td>15.68824</td></tr><tr><td>valid_dist</td><td>573.4375</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">Attention-Model-Beta</strong> at: <a href='https://wandb.ai/idl-f23-bestteam/IDL_group_project/runs/7xo0wsay' target=\"_blank\">https://wandb.ai/idl-f23-bestteam/IDL_group_project/runs/7xo0wsay</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231211_230557-7xo0wsay/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:7xo0wsay). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231211_234920-f3csj8zi</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/idl-f23-bestteam/IDL_group_project/runs/f3csj8zi' target=\"_blank\">Attention-Model-Beta</a></strong> to <a href='https://wandb.ai/idl-f23-bestteam/IDL_group_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/idl-f23-bestteam/IDL_group_project' target=\"_blank\">https://wandb.ai/idl-f23-bestteam/IDL_group_project</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/idl-f23-bestteam/IDL_group_project/runs/f3csj8zi' target=\"_blank\">https://wandb.ai/idl-f23-bestteam/IDL_group_project/runs/f3csj8zi</a>"]},"metadata":{}}],"source":["run = wandb.init(\n","    name = \"Attention-Model-Beta\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"IDL_group_project\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"],"id":"Aw1BG-EzshMj"},{"cell_type":"code","source":["import seaborn as sns\n","\n","def plot_attention(attention):\n","    # Function for plotting attention\n","    # You need to get a diagonal plot\n","    plt.clf()\n","    sns.heatmap(attention, cmap='GnBu')\n","    plt.show()"],"metadata":{"id":"FngkGzw1E5fB","executionInfo":{"status":"ok","timestamp":1702338566893,"user_tz":300,"elapsed":19,"user":{"displayName":"Colton Gonsisko","userId":"07035002796290559229"}}},"id":"FngkGzw1E5fB","execution_count":58,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4w7yRrasVrs","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["347e1e8f8ca94100b6b4a4dacfe0d7e6","6acb0fa07bb9480bb45a6d7528c1db22","6c177b6ecff746eea35591fa6b04c15a","8238d2383df342939a42033adda14fb7","03d48bf603fe4c8b91aa40a347598a87","f051dd37d97d49f68f95e9510a905767","0477327bf3fe41909c6cc5b6b6f6c4c1","0c264eba83a04dd0af5e36c7db0aa2a7","05063883b0694239938364a88fa2741f","dfb15a2e4f0c4340a5e79a90fd7edcc6","47b626a946de41e080f7095274bfef8f","2797971effd743f38e3cd38138a69cb0","b48e4d4626b140109227e7312a7084ca","9a959b8774c14d09ba8900ffd8e56e71","910cecdf1a104c31bf9458dc09011337","6106d86f0b904d6eabe4c37f3bf251a5","e3a549ad3e7e47ccb67c981b1329a5c1","a47c6bc29fed4b748ce8277af2f3b09e","96da98da68a94d078df22e78360d84db","a53115e4d95348129ac9c6c9daa557e2","1513e530e972421195a3947d7d6aa61c","6411e8421bf0432eae4dbada5b52c8f1","95bcb73ab16e4d0482a4c4b143084889","c5178e5dee0146b4bec157c749312a60","c247adac79ce464d8685c2679e23847d","81c018d184aa4c27b1215a9def667c29","9bcd18f4a45c41509ef092a018ddf9a9","d6d4c8a9b36d4b0fa660cbcebcd6c4e1","2df5a998420e460b9dc18f8ec075ba05","2b48a1c7746e4026a1744f6006ce41d2","bb79b9fddc9443f1a793b8a7264f0db0","ead4c0baf36b4844bfc83688a5a56131","1d68315b627446459f35f0df419e183e","efab3a0634c94811a5fe81c0b8b93587","843e54ec1c3a4ec198b96392966daede","86cbe220cc2a4015a3de006b89722642","1b331d8ef1004cd782be5704b64498a0","6c2d15e42d0d443ba03fadd366b48821","2bcc4330378246d8a44b17ab97ecbced","c223d49983334df18347a7bf25402fd6","18a39aad2cb14597bb91bf6bcc9dfcef","3392f1be82414122b39589ce8aacbcbe","950de40a02064595bdc977c4f2fafea5","8986564d527840d4996e813a8131dd93","830ace2204bd4a5893ca42a89f551fbb","ef0e6495b69b43e9957a093c8a6e0768","c2e8ba552d89423e9fffe81496869317","893e669a9cb24206b8e9593b754b9b5e","c6718b6f969941858bc41b840349d532","a851b96d4aa049fdb7974b06f19fccc2","bceb0c294548458493cf7aa8a8a654c4","af691ea12fdf4a5792f377da376b76df","ed5be04cb23d4637aad17fc1948cfc92","238884d2def0483ea32fe9f8420415ad","dc19064f9be74819a0ebcb9cb5b4a24e","fcbae7bf44634709bfd8c86dc82bef89","0956dbf147bb45db909bebf5b64b08cc","e18d800d018345feb8d868deab04ef10","79e58bac045c4400bae01663f6423007","dcd809ea86b94a7a99b1f58a54c4ffcc","32edc33218b84034a5c42fed062c59e9","80168ee141c84d44bee11766d79aedbf","3dc070a551c44fd39d350be1ea68eff8","2deed64c425746efaf35ceeb851df889","e0b8e1223ea4412b904df9be16646310","d4cbf7586d994849a5152e75e6ac0f99","8f7795faf0514a1c9b602b2f8ffba96e","8a302a9c351e4f748493ec1ccc026792","6af9c6fc78354147ba3363f95f88aa3e","9cc130e3beac40cab2de44105968eadf","1522525649a146268d5beb461a2e8c04","3d4943a415e04b76a848b420fe54202d","ab0964439fa14ddda6178f9d8d14e857","f2702a28122b4f2aaa5b3b29a864152d","8cc5c95724d3486fa75edf6314613549","0e8012f8cd504a06b07bd36236e6e166","6c81d855469e46ffbe443590f87139a8","06eba2fa856e42ad8f246f05884c60df","2b63482c83de4d12898c4a2107f4f3d9","61973b8bde1f4c0e8eefbc3787956e95","48f6b28d91f3420f82255e0a5e6ae98d","868430b3093e4fe6897cce37ee7d794c","97b39a651e304b0e991c0aa83f3ba25b","d661d01ff16144d297dcbdf55d9fd8d0","8b9b68f834fe4a9c8827abc68919b403","b7a9f6d5936d4a4ab6705f331f1378e4","e197a4773d9b4f1ba961683aa135fa5b","50e58e2770cd41d5a0ed842cad7bc8d9","0b128231d95c4223aff0b88b19628e54","cbd1f11bf6d7464aa1a05cda462344df","3e9d93bf720045bbb8ea2962abb6f648","ae77ed71c66c48c292d5c95f1f032d35","097fe2044f6c45b09a2a0e2f91a1e175","af720319750e493ab3606cd37a9af32b","1c4a6e3c20144e848d2e6f631e81f2ef","c0de5014a07e40c582cd7170d507384d","09516be256004e32ae020e3b9919ad26","f0b88ff73b7d45e19b358f5059f8d959","07fd14cf9d784276aaee4e7c41b97a75"],"output_embedded_package_id":"1Z6R0k_FPEsavIGScFiIKCecTVMQ541bA"},"outputId":"c49ebb3b-8800-4856-ca75-a27734c01462"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","best_lev_dist = float(\"inf\")\n","tf_rate = 1.0\n","path = r\"./checkpoint.pth\"\n","tf_scheduler = None\n","\n","for epoch in range(0, config['epochs']):\n","\n","    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n","\n","    # Call train and validate, get attention weights from training\n","    running_loss, running_perplexity, attention_plot = train(model, train_loader, criterion, optimizer, tf_rate)\n","\n","    # Print your metrics\n","    print('[TRAIN] \\t Loss: %.4f \\t Perplexity: %.4f \\tLr: %.6f'\n","          % (running_loss, running_perplexity, optimizer.param_groups[0]['lr']))\n","\n","\n","    running_lev_dist = validate(model, val_loader)\n","    print('[VAL] \\tLoss: %.4f'% (running_lev_dist))\n","\n","    # Plot Attention for a single item in the batch\n","    plot_attention(attention_plot[0].detach().cpu())\n","\n","    # Log metrics to Wandb\n","    wandb.log({\n","        'train_loss': running_loss,\n","        'train_perplex': running_perplexity,\n","        'valid_dist': running_lev_dist,\n","        'lr'        : optimizer.param_groups[0]['lr'],\n","    })\n","\n","    # Optional: Scheduler Step / Teacher Force Schedule Step\n","    scheduler.step(running_lev_dist)\n","\n","    if running_lev_dist <= best_lev_dist:\n","        best_lev_dist = running_lev_dist\n","        metric = [\"valid_dist\", running_lev_dist]\n","        save_model(model, optimizer, scheduler, tf_scheduler, metric, epoch, path)\n","        wandb.save(os.path.join(wandb.run.dir, \"checkpoint*\"))\n","        print(\"Saved best model\")\n","        # Save your model checkpoint here"],"id":"H4w7yRrasVrs"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0fe76819e05343d0a12cc5eb9a5169d2":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5ed131cc93454ae899962a15af5889fd","IPY_MODEL_1bbec8042c5c4eef874465679e6900cf"],"layout":"IPY_MODEL_5e05f309b6014433b7975a2fd6daac57"}},"5ed131cc93454ae899962a15af5889fd":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd45fd8585b3462ba8643ad0d0033300","placeholder":"​","style":"IPY_MODEL_90fa39a042bc4cbda1dc1f7fac70060c","value":"0.063 MB of 0.063 MB uploaded\r"}},"1bbec8042c5c4eef874465679e6900cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec27ce4efd414a89bdd831bcdf795dcf","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4ddee77ad26465d846b80b363005d5a","value":1}},"5e05f309b6014433b7975a2fd6daac57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd45fd8585b3462ba8643ad0d0033300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90fa39a042bc4cbda1dc1f7fac70060c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec27ce4efd414a89bdd831bcdf795dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ddee77ad26465d846b80b363005d5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"347e1e8f8ca94100b6b4a4dacfe0d7e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6acb0fa07bb9480bb45a6d7528c1db22","IPY_MODEL_6c177b6ecff746eea35591fa6b04c15a","IPY_MODEL_8238d2383df342939a42033adda14fb7"],"layout":"IPY_MODEL_03d48bf603fe4c8b91aa40a347598a87"}},"6acb0fa07bb9480bb45a6d7528c1db22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f051dd37d97d49f68f95e9510a905767","placeholder":"​","style":"IPY_MODEL_0477327bf3fe41909c6cc5b6b6f6c4c1","value":"Train:  92%"}},"6c177b6ecff746eea35591fa6b04c15a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c264eba83a04dd0af5e36c7db0aa2a7","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05063883b0694239938364a88fa2741f","value":12}},"8238d2383df342939a42033adda14fb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfb15a2e4f0c4340a5e79a90fd7edcc6","placeholder":"​","style":"IPY_MODEL_47b626a946de41e080f7095274bfef8f","value":" 12/13 [01:34&lt;00:07,  7.63s/it, loss=4.8634, lr=0.0001, perplexity=909.4348, tf_rate=1.00]"}},"03d48bf603fe4c8b91aa40a347598a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"f051dd37d97d49f68f95e9510a905767":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0477327bf3fe41909c6cc5b6b6f6c4c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c264eba83a04dd0af5e36c7db0aa2a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05063883b0694239938364a88fa2741f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfb15a2e4f0c4340a5e79a90fd7edcc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47b626a946de41e080f7095274bfef8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2797971effd743f38e3cd38138a69cb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b48e4d4626b140109227e7312a7084ca","IPY_MODEL_9a959b8774c14d09ba8900ffd8e56e71","IPY_MODEL_910cecdf1a104c31bf9458dc09011337"],"layout":"IPY_MODEL_6106d86f0b904d6eabe4c37f3bf251a5"}},"b48e4d4626b140109227e7312a7084ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3a549ad3e7e47ccb67c981b1329a5c1","placeholder":"​","style":"IPY_MODEL_a47c6bc29fed4b748ce8277af2f3b09e","value":"Val:  75%"}},"9a959b8774c14d09ba8900ffd8e56e71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_96da98da68a94d078df22e78360d84db","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a53115e4d95348129ac9c6c9daa557e2","value":3}},"910cecdf1a104c31bf9458dc09011337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1513e530e972421195a3947d7d6aa61c","placeholder":"​","style":"IPY_MODEL_6411e8421bf0432eae4dbada5b52c8f1","value":" 3/4 [00:16&lt;00:05,  5.17s/it, dist=810.6204]"}},"6106d86f0b904d6eabe4c37f3bf251a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e3a549ad3e7e47ccb67c981b1329a5c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47c6bc29fed4b748ce8277af2f3b09e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96da98da68a94d078df22e78360d84db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a53115e4d95348129ac9c6c9daa557e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1513e530e972421195a3947d7d6aa61c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6411e8421bf0432eae4dbada5b52c8f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95bcb73ab16e4d0482a4c4b143084889":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5178e5dee0146b4bec157c749312a60","IPY_MODEL_c247adac79ce464d8685c2679e23847d","IPY_MODEL_81c018d184aa4c27b1215a9def667c29"],"layout":"IPY_MODEL_9bcd18f4a45c41509ef092a018ddf9a9"}},"c5178e5dee0146b4bec157c749312a60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6d4c8a9b36d4b0fa660cbcebcd6c4e1","placeholder":"​","style":"IPY_MODEL_2df5a998420e460b9dc18f8ec075ba05","value":"Train:  92%"}},"c247adac79ce464d8685c2679e23847d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b48a1c7746e4026a1744f6006ce41d2","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb79b9fddc9443f1a793b8a7264f0db0","value":12}},"81c018d184aa4c27b1215a9def667c29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ead4c0baf36b4844bfc83688a5a56131","placeholder":"​","style":"IPY_MODEL_1d68315b627446459f35f0df419e183e","value":" 12/13 [01:35&lt;00:07,  7.80s/it, loss=2.9997, lr=0.0000, perplexity=20.0802, tf_rate=1.00]"}},"9bcd18f4a45c41509ef092a018ddf9a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d6d4c8a9b36d4b0fa660cbcebcd6c4e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df5a998420e460b9dc18f8ec075ba05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b48a1c7746e4026a1744f6006ce41d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb79b9fddc9443f1a793b8a7264f0db0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ead4c0baf36b4844bfc83688a5a56131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d68315b627446459f35f0df419e183e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efab3a0634c94811a5fe81c0b8b93587":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_843e54ec1c3a4ec198b96392966daede","IPY_MODEL_86cbe220cc2a4015a3de006b89722642","IPY_MODEL_1b331d8ef1004cd782be5704b64498a0"],"layout":"IPY_MODEL_6c2d15e42d0d443ba03fadd366b48821"}},"843e54ec1c3a4ec198b96392966daede":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bcc4330378246d8a44b17ab97ecbced","placeholder":"​","style":"IPY_MODEL_c223d49983334df18347a7bf25402fd6","value":"Val:  75%"}},"86cbe220cc2a4015a3de006b89722642":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a39aad2cb14597bb91bf6bcc9dfcef","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3392f1be82414122b39589ce8aacbcbe","value":3}},"1b331d8ef1004cd782be5704b64498a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_950de40a02064595bdc977c4f2fafea5","placeholder":"​","style":"IPY_MODEL_8986564d527840d4996e813a8131dd93","value":" 3/4 [00:16&lt;00:05,  5.15s/it, dist=810.6204]"}},"6c2d15e42d0d443ba03fadd366b48821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"2bcc4330378246d8a44b17ab97ecbced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c223d49983334df18347a7bf25402fd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18a39aad2cb14597bb91bf6bcc9dfcef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3392f1be82414122b39589ce8aacbcbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"950de40a02064595bdc977c4f2fafea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8986564d527840d4996e813a8131dd93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"830ace2204bd4a5893ca42a89f551fbb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef0e6495b69b43e9957a093c8a6e0768","IPY_MODEL_c2e8ba552d89423e9fffe81496869317","IPY_MODEL_893e669a9cb24206b8e9593b754b9b5e"],"layout":"IPY_MODEL_c6718b6f969941858bc41b840349d532"}},"ef0e6495b69b43e9957a093c8a6e0768":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a851b96d4aa049fdb7974b06f19fccc2","placeholder":"​","style":"IPY_MODEL_bceb0c294548458493cf7aa8a8a654c4","value":"Train:  92%"}},"c2e8ba552d89423e9fffe81496869317":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_af691ea12fdf4a5792f377da376b76df","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed5be04cb23d4637aad17fc1948cfc92","value":12}},"893e669a9cb24206b8e9593b754b9b5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_238884d2def0483ea32fe9f8420415ad","placeholder":"​","style":"IPY_MODEL_dc19064f9be74819a0ebcb9cb5b4a24e","value":" 12/13 [01:37&lt;00:08,  8.07s/it, loss=2.9998, lr=0.0000, perplexity=20.0815, tf_rate=1.00]"}},"c6718b6f969941858bc41b840349d532":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a851b96d4aa049fdb7974b06f19fccc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bceb0c294548458493cf7aa8a8a654c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af691ea12fdf4a5792f377da376b76df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed5be04cb23d4637aad17fc1948cfc92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"238884d2def0483ea32fe9f8420415ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc19064f9be74819a0ebcb9cb5b4a24e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcbae7bf44634709bfd8c86dc82bef89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0956dbf147bb45db909bebf5b64b08cc","IPY_MODEL_e18d800d018345feb8d868deab04ef10","IPY_MODEL_79e58bac045c4400bae01663f6423007"],"layout":"IPY_MODEL_dcd809ea86b94a7a99b1f58a54c4ffcc"}},"0956dbf147bb45db909bebf5b64b08cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32edc33218b84034a5c42fed062c59e9","placeholder":"​","style":"IPY_MODEL_80168ee141c84d44bee11766d79aedbf","value":"Val:  75%"}},"e18d800d018345feb8d868deab04ef10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dc070a551c44fd39d350be1ea68eff8","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2deed64c425746efaf35ceeb851df889","value":3}},"79e58bac045c4400bae01663f6423007":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0b8e1223ea4412b904df9be16646310","placeholder":"​","style":"IPY_MODEL_d4cbf7586d994849a5152e75e6ac0f99","value":" 3/4 [00:18&lt;00:06,  6.01s/it, dist=810.6204]"}},"dcd809ea86b94a7a99b1f58a54c4ffcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"32edc33218b84034a5c42fed062c59e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80168ee141c84d44bee11766d79aedbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3dc070a551c44fd39d350be1ea68eff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2deed64c425746efaf35ceeb851df889":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e0b8e1223ea4412b904df9be16646310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4cbf7586d994849a5152e75e6ac0f99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f7795faf0514a1c9b602b2f8ffba96e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a302a9c351e4f748493ec1ccc026792","IPY_MODEL_6af9c6fc78354147ba3363f95f88aa3e","IPY_MODEL_9cc130e3beac40cab2de44105968eadf"],"layout":"IPY_MODEL_1522525649a146268d5beb461a2e8c04"}},"8a302a9c351e4f748493ec1ccc026792":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d4943a415e04b76a848b420fe54202d","placeholder":"​","style":"IPY_MODEL_ab0964439fa14ddda6178f9d8d14e857","value":"Train:  92%"}},"6af9c6fc78354147ba3363f95f88aa3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2702a28122b4f2aaa5b3b29a864152d","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cc5c95724d3486fa75edf6314613549","value":12}},"9cc130e3beac40cab2de44105968eadf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8012f8cd504a06b07bd36236e6e166","placeholder":"​","style":"IPY_MODEL_6c81d855469e46ffbe443590f87139a8","value":" 12/13 [01:34&lt;00:07,  7.67s/it, loss=3.0001, lr=0.0000, perplexity=20.0884, tf_rate=1.00]"}},"1522525649a146268d5beb461a2e8c04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3d4943a415e04b76a848b420fe54202d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab0964439fa14ddda6178f9d8d14e857":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2702a28122b4f2aaa5b3b29a864152d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cc5c95724d3486fa75edf6314613549":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e8012f8cd504a06b07bd36236e6e166":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c81d855469e46ffbe443590f87139a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06eba2fa856e42ad8f246f05884c60df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b63482c83de4d12898c4a2107f4f3d9","IPY_MODEL_61973b8bde1f4c0e8eefbc3787956e95","IPY_MODEL_48f6b28d91f3420f82255e0a5e6ae98d"],"layout":"IPY_MODEL_868430b3093e4fe6897cce37ee7d794c"}},"2b63482c83de4d12898c4a2107f4f3d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97b39a651e304b0e991c0aa83f3ba25b","placeholder":"​","style":"IPY_MODEL_d661d01ff16144d297dcbdf55d9fd8d0","value":"Val:  75%"}},"61973b8bde1f4c0e8eefbc3787956e95":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b9b68f834fe4a9c8827abc68919b403","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7a9f6d5936d4a4ab6705f331f1378e4","value":3}},"48f6b28d91f3420f82255e0a5e6ae98d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e197a4773d9b4f1ba961683aa135fa5b","placeholder":"​","style":"IPY_MODEL_50e58e2770cd41d5a0ed842cad7bc8d9","value":" 3/4 [00:16&lt;00:05,  5.20s/it, dist=810.6204]"}},"868430b3093e4fe6897cce37ee7d794c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"97b39a651e304b0e991c0aa83f3ba25b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d661d01ff16144d297dcbdf55d9fd8d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b9b68f834fe4a9c8827abc68919b403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7a9f6d5936d4a4ab6705f331f1378e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e197a4773d9b4f1ba961683aa135fa5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e58e2770cd41d5a0ed842cad7bc8d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b128231d95c4223aff0b88b19628e54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbd1f11bf6d7464aa1a05cda462344df","IPY_MODEL_3e9d93bf720045bbb8ea2962abb6f648","IPY_MODEL_ae77ed71c66c48c292d5c95f1f032d35"],"layout":"IPY_MODEL_097fe2044f6c45b09a2a0e2f91a1e175"}},"cbd1f11bf6d7464aa1a05cda462344df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af720319750e493ab3606cd37a9af32b","placeholder":"​","style":"IPY_MODEL_1c4a6e3c20144e848d2e6f631e81f2ef","value":"Train:   8%"}},"3e9d93bf720045bbb8ea2962abb6f648":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0de5014a07e40c582cd7170d507384d","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09516be256004e32ae020e3b9919ad26","value":1}},"ae77ed71c66c48c292d5c95f1f032d35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0b88ff73b7d45e19b358f5059f8d959","placeholder":"​","style":"IPY_MODEL_07fd14cf9d784276aaee4e7c41b97a75","value":" 1/13 [00:15&lt;01:42,  8.54s/it, loss=3.0338, lr=0.0000, perplexity=20.7848, tf_rate=1.00]"}},"097fe2044f6c45b09a2a0e2f91a1e175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"af720319750e493ab3606cd37a9af32b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4a6e3c20144e848d2e6f631e81f2ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0de5014a07e40c582cd7170d507384d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09516be256004e32ae020e3b9919ad26":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0b88ff73b7d45e19b358f5059f8d959":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07fd14cf9d784276aaee4e7c41b97a75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}